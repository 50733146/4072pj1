<!DOCTYPE html><html>
        <head>
        <title>Reinforcement Learning in Mechatroinc Systems</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700,900" rel="stylesheet">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/fonts/icomoon/style.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/bootstrap.min.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/magnific-popup.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/jquery-ui.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/owl.carousel.min.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/owl.theme.default.min.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/bootstrap-datepicker.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/fonts/flaticon/font/flaticon.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/aos.css">
        <link rel="stylesheet" href="./../cmsimde/static/chimper/css/style.css">
        <link rel="shortcut icon" href="./../cmsimde/static/favicons.png">
        
        <style type='text/css'>
            .site-section {
            background-color: #FFFF;
            padding: 40px 40px;
            }
            body > div > div.dropdown.open {
                display: block;
            }
        </style>
    
        <!-- <script src="./../cmsimde/static/jquery.js"></script> -->
        <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script> -->
        <script src="../cmsimde/static/chimper/js/jquery-3.3.1.min.js"></script>
        <link rel="stylesheet" href="./../cmsimde/static/tipuesearch/css/normalize.min.css">
        <script src="./../cmsimde/static/tipuesearch/tipuesearch_set.js"></script>
        <script src="tipuesearch_content.js"></script>
        <link rel="stylesheet" href="./../cmsimde/static/tipuesearch/css/tipuesearch.css">
        <script src="./../cmsimde/static/tipuesearch/tipuesearch.js"></script>
        <script>
            /* original tipuesearch
            $(document).ready(function() {
                 $('#tipue_search_input').tipuesearch();
            });
            */
            // customed doSearch
            function doSearch() {
                $('#tipue_search_input').tipuesearch({
                    newWindow: true, 
                    minimumLength: 2,
                    wholeWords: false, // for search 中文
                });
            }
            $(document).ready(doSearch);
        </script>
        
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shCore.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushBash.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushDiff.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushJScript.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushJava.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushPython.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushSql.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushHaxe.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushXml.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushPhp.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushLua.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushCpp.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushCss.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushCSharp.js"></script>
<script type="text/javascript" src="./../cmsimde/static/syntaxhighlighter/shBrushDart.js"></script>
<link type="text/css" rel="stylesheet" href="./../cmsimde/static/syntaxhighlighter/css/shCoreDefault.css"/>
<script type="text/javascript">SyntaxHighlighter.all();</script>
<!-- 暫時不用
<script src="./../cmsimde/static/fengari-web.js"></script>
<script type="text/javascript" src="./../cmsimde/static/Cango-13v08-min.js"></script>
<script type="text/javascript" src="./../cmsimde/static/CangoAxes-4v01-min.js"></script>
<script type="text/javascript" src="./../cmsimde/static/gearUtils-05.js"></script>
-->
<!-- for Brython 暫時不用
<script src="https://scrum-3.github.io/web/brython/brython.js"></script>
<script src="https://scrum-3.github.io/web/brython/brython_stdlib.js"></script>
-->
<style>
img.add_border {
    border: 3px solid blue;
}
</style>

</head>
<body>
<div class='container'><nav>
        
    <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div>
    
            <header class="site-navbar py-4 bg-white" role="banner">
              <div class="container-fluid">
                <div class="row align-items-center">
                <h1>強化學習在機電系統設計與控制中之應用 </h1>
                <div class="pl-4">
                    <form>
                    <input type="text" placeholder="Search" name="q" id="tipue_search_input" pattern=".{2,}" title="At least 2 characters" required>
                    </form>
                </div>
                  <!-- <div class="col-11 col-xl-2">
                    <h1 class="mb-0 site-logo"><a href="index.html" class="text-black h2 mb-0">強化學習在機電系統設計與控制中之應用 </a></h1> 
                  </div>
                  -->
                  <div class="col-12 col-md-10 d-none d-xl-block">
                    <nav class="site-navigation position-relative text-right" role="navigation">
    <ul class='site-menu js-clone-nav mr-auto d-none d-lg-block'>
                        <li class="active has-children"><a href="index.html">Home</a>
                        <ul class="dropdown">
                            <li><a href="sitemap.html">Site Map</a></li>
                            <li><a href="./../reveal/index.html">reveal</a></li>
                            <li><a href="./../blog/index.html">blog</a></li>
                        </ul>
                      </li>
                     <li class='has-children'><a href='About.html'>About</a><ul class='dropdown'><li><a href='專題定位.html'>專題定位</a><li><a href='LaTeX.html'>LaTeX</a><li><a href='參考資料.html'>參考資料</a><li><a href='研究生考試.html'>研究生考試</a></li></ul><li class='has-children'><a href='動態網站.html'>動態網站</a><ul class='dropdown'><li><a href='數位簽章.html'>數位簽章</a><li><a href='機電控制.html'>機電控制</a><li><a href='iRobot Create.html'>iRobot Create</a></li></ul><li class='has-children'><a href='深度學習.html'>深度學習</a><ul class='dropdown'><li><a href='電腦.html'>電腦</a><li><a href='Flask.html'>Flask</a></li></ul><li class='has-children'><a href='強化學習.html'>強化學習</a><ul class='dropdown'><li><a href='2021.html'>2021</a><li><a href='Reference.html'>Reference</a><li class='has-children'><a href='類神經網路學習.html'>類神經網路學習</a><ul class='dropdown'><li><a href='neural_network_in_python.pdf.html'>neural_network_in_python.pdf</a></li></ul><li><a href='參考範例.html'>參考範例</a><li><a href='Markov Decision Process.html'>Markov Decision Process</a></li></ul><li class='has-children'><a href='優化器.html'>優化器</a><ul class='dropdown'><li><a href='Gradient Descent Optimizer.html'>Gradient Descent Optimizer</a><li><a href='Stochastic gradient descent.html'>Stochastic gradient descent</a><li><a href='Gradient descent optimization algorithms.html'>Gradient descent optimization algorithms</a></li></ul><li class='has-children'><a href='程式.html'>程式</a><ul class='dropdown'><li><a href='RL-Pong Game.html'>RL-Pong Game</a></li></ul><li class='has-children'><a href='Ref.html'>Ref</a><ul class='dropdown'><li><a href='Flutter.html'>Flutter</a><li><a href='CMSiMDE.html'>CMSiMDE</a></li>
                      </ul>
                </nav>
              </div>
              <div class="d-inline-block d-xl-none ml-md-0 mr-auto py-3" style="position: relative; top: 3px;"><a href="#" class="site-menu-toggle js-menu-toggle text-black"><span class="icon-menu h3"></span></a></div>
              </div>

            </div>
          </div>
          
        </header>
    <div id="tipue_search_content">Stochastic gradient descent << <a href='Stochastic gradient descent.html'>Previous</a> <a href='程式.html'>Next</a> >> 程式<br /><h1>Gradient descent optimization algorithms</h1>
<p><span>(</span><a href="https://ruder.io/optimizing-gradient-descent/">資料來源</a><span>)</span></p>
<h4>Momentum</h4>
<p>SGD 難以在陡峭的往正確的方向，那就是說在一個維度上，曲面的彎曲比另一個維度要陡得多，這在局部最優情況下很常見。下圖[圖.1]的同心圓代表中心下凹的曲面。在這些情況下，SGD 會在陡峭的地方振盪，而僅沿著底部朝著局部最優方向猶豫前進，如 [圖.1.a] 所 示。 Momentun(動量) 是一個幫助加速 SGD 在正確方向和抑制震盪的方法，在 [圖.1.b]。</p>
<table style="width: 250.667px; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="width: 236px;"><img alt="without_momentum" caption="false" height="100" src="./../images/without_momentum.png" width="236"/></td>
<td style="width: 10px;"><img alt="with_momentum" caption="false" height="100" src="./../images/with_momentum.png" width="236"/></td>
</tr>
<tr>
<td style="width: 236px; text-align: center;"><span> 圖.1.</span>a SGD without momentum <sub><span>(</span><a href="https://ruder.io/optimizing-gradient-descent/">圖片來源</a><span>)</span></sub></td>
<td style="width: 10px; text-align: center;"><span>圖.1.</span>b SGD with momentum <sub>(<a href="https://ruder.io/optimizing-gradient-descent/">圖片來源</a>)</sub></td>
</tr>
</tbody>
</table>
<p style="text-align: left;">這麼做會增加一個係數 γ (gamma) 來更新上次的向量到正確向量 (修正偏差)，γ 通常設為 0.9 左右。</p>
<p style="text-align: center;">v<sub>t</sub> = γv<sub>t−1</sub> + η·▽θJ(θ) <br/>θ = θ − v<sub>t</sub></p>
<p style="text-align: left;">實際上，使用動量的時候，就像將球推下山坡。球在下坡時滾動時會累積動量，在途中速度會越來越快（如果存在空氣阻力，直到達到極限速度，也就是 γ &lt; 1) 參數更新也發生了同樣的事情：動量 (momentum) 對於梯度指向相同方向的維度增加，而對於梯度改變方向的維減少動量。結果，我們獲得了更快的收斂並減少了振盪。</p>
<h4 style="text-align: left;">Nesterov accelerated gradient</h4>
<p>Nesterov accelerated gradient（NAG）是一種使動量具有一個去向的概念，以便在山坡再次變高之前知道它會減速。我們知道使用動量 γv<sub>t−1</sub> 來移動參數。計算 θ − γv<sub>t−1</sub> 這樣就給了參數的下一個位置的近似值（完整更新缺少的梯度），這是參數將要存在的大致概念。現在，通過計算與當前參數無關的梯度來有效地看到目前的參數 θ 將會移動到的位置：</p>
<p style="text-align: center;"><span>v</span><sub>t</sub> = γvt−1 + η·▽θJ(θ−γvt−1)</p>
<p style="text-align: center;">θ = θ − v<sub>t</sub></p>
<p style="text-align: center;"><sub><img alt="" height="100" src="./../images/NAG.jpg" width="345"/></sub></p>
<p style="text-align: center;">圖.2 NAG <sub>(<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">圖片來源</a>)</sub></p>
<p>同樣，我們設置動量 γ 約為 0.9。動量首先計算當前梯度（[圖.2] 中的藍色小向量），然後在更新的累積梯度（藍 色向量）的方向上發生較大的跳躍，而 NAG 首先在先前的累積梯度的方向上進行較大的跳躍（棕色向量），測量梯度，然後進行校正（紅色向量），從而完成 NAG 更新（綠色向量）。這種預期的更新可防止我們過快地進行，並導致響應速度增加，從而顯著提高了 RNN 在許多任務上的性能。有關 NAG 背後另一解釋，請<a href="https://cs231n.github.io/neural-networks-3/">參見此處</a>，而 Ilya Sutskever 在其博士論文中給出了更詳細的概述。</p>
<h4>Adagrad</h4>
<p>Adagrad 是一個梯度優化的算法，它可以做到：學習率適應參數，對於頻繁出現的特徵相關參數執行較小的更新(較低的學習率)，以及對不經常出現的特徵相關參數進行較大更新（即學習率較高）。Adagrad 可以提高 SGD 的強度，用於訓練大型神經網絡。</p>
<p>先前，在同一次 θ 參數(更新後就算另一次)，每個 θ 都使用相同的 η (學習率)。Adagrad 則是對每個 θ 參數使用 不同的 η，t 代表 time step。先將 Adagrad 的更新參數向量化。用 g<sub>t</sub> 表示 time step 的梯度，g<sub>t,i</sub> 表示目標函數 (參數 θ 在 time step t) 對參數做偏微分計算。</p>
<p style="text-align: center;">g<sub>t,i</sub> = ▽θJ(θ<sub>t,i </sub>)</p>
<p>當 SGD 更新每個參數 θ<sub>i</sub>，在每個 time step t，因此變成：</p>
<p style="text-align: center;">θ<sub>t+1,i</sub> = θ<sub>t,i</sub> − η · g<sub>t,i </sub></p>
<p>更新規則，Adagrad 根據先前 θ<sub>i</sub> 計算的梯度，對每個參數 θ<sub>i</sub> 修改整個學習率 η 在每個 time stept：</p>
<p style="text-align: center;">θ<sub>t+1,i</sub> = θ<sub>t,i</sub> − [η / (G<sub>t,ii</sub> + ϵ)<sup>½</sup>]</p>
<p>G<sub>t</sub> ∈ R<sup>d×d</sup> 這是一個對角矩陣每個對角元素 i，i 是關於 θ 梯度平方和取決於 time stept，ϵ 是避免分母為 0(ϵ 通常 為 10<sup>−8</sup> )，如果沒有平方根運算，該算法的性能將大大降低。 G<sub>t</sub> 包含了過去梯度平方根，由於全部 θ 參數沿著對角線，通過向量的內積計算 G<sub>t</sub> 和 g<sub>t</sub>：</p>
<p style="text-align: center;">θ<sub>t+1</sub> = θ<sub>t</sub> −<span> [η / (G</span><sub>t </sub><span>+ ϵ)</span><sup>½</sup><span>]</span>·g<sub>t </sub></p>
<p>Adagrad 主要好處之一是，無需手動調整學習率。大多數實現使用預設值 0.01 並將其保留為預設值。Adagrad 主 要弱點是會累積分母的平方梯度：由於每項都是正的，累積和會在訓練中不斷增長。反過來，學習率下降，並最終變 得無限小，這算法就不再獲得知識。</p>
<h4><span>Adadelta</span></h4>
<p>Adadelta 是 Adagrad 的延伸，下降其激進的程度，單調的降低學習率。Adadelta 會限制過去累積的梯度，並將其 限制在某個特定大小 w，並代替 Adagrad 過去累積的梯度平方，以梯度總和是遞迴定義為所有過去衰減梯度平方平均值。流動平均 E[g<sup><sub>2</sub></sup>]<sub>t</sub> 在 time step t 然後取決於 (像 Momentum 的 γ) 先前 平均和最近梯度：</p>
<p style="text-align: center;"><span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t</sub> = γ<span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t</sub><sub>-1</sub> + (1 − γ)g<sup>2</sup><sub>t</sub></p>
<p>γ 值和 Momentum 的相似，約為 0.9，現在根據參數更新向量 △θt 來重寫 SGD：</p>
<p style="text-align: center;">△θ<sub>t</sub> = −η · g<sub>t,i</sub></p>
<p style="text-align: center;">θ<sub>t+1</sub> = θ<sub>t</sub> + △θ<sub>t</sub></p>
<p>Adagrad 的參數更新向量替換成：對角矩陣 G<sub>t</sub> 過去梯度平方的衰退平均 <span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t</sub></p>
<p style="text-align: center;"><span>△θ</span><sub>t</sub> = − <span>[η / (G</span><sub>t </sub><span>+ ϵ)</span><sup>½</sup><span>]</span><span>·g</span><sub>t</sub></p>
<p style="text-align: center;">replace <span>G</span><sub>t</sub><span><span> </span></span>with <span><span> </span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t</sub><span><span> </span></span> ⇒ △<span>θ</span><sub>t</sub> = <span>−</span><span><span> </span></span><span>[η / (E[g<sup><sub>2</sub></sup><span>]</span><sub>t </sub></span><span>+ ϵ)</span><sup>½</sup><span>]</span><span>·g</span><sub>t</sub></p>
<p>由於分母只是梯度的均方根 (RMS)，我們可以取代成縮寫：</p>
<p style="text-align: center;"><span>△θ</span><sub>t</sub> = − η RMS[g]<sub>t</sub> · <span>g</span><sub>t</sub></p>
<p>這個更新單位和 SGD、Momentum 以及 Adagrad 的單位不符合，因此更新需有相同的參數。為了實現這一點，首先定義另一個指數衰減平均值，這次不是梯度平方更新而是參數平方更新：</p>
<p style="text-align: center;">E[<span>△θ<sup>2</sup></span>]<sub>t</sub> = γE[△θ<sup>2</sup>]<sub>t−1</sub> + (1 − γ) △ θ<sup>2</sup><sub>t</sub></p>
<p>RMS 參數更新:</p>
<p style="text-align: center;">RMS[△θ<span>]</span><sub>t</sub> = (E<span>[△θ</span><sup>2</sup><span>]</span><sub>t</sub> + ϵ<span>)</span><sup>½</sup></p>
<p>RMS[△θ<span>]</span><sub>t</sub> 是未知的，更新參數的 RMS 取近似值到上個 time step。用 RMS<span>[△θ</span><span>]</span><sub>t-1</sub> 取代學習率 η，最後產生新 的規則：</p>
<p style="text-align: center;"><span>△</span><span>θ</span><sub>t</sub> = − (RMS[△θ<span>]</span><sub>t-1</sub> / RMS[g<span>]</span><sub>t</sub> )<span>·g</span><sub>t<br/></sub><span>θ</span><sub>t-1</sub>= θ<sub>t</sub> + △<span>θ</span><sub>t</sub></p>
<p>使用 Adadelta，甚至不需要設定預設學習率，因為它已從更新規則淘汰。</p>
<h4>RMSprop</h4>
<p>RMSprop 是 Geoffrey Hinton 在他的課程中提出的未公開自適應學習率的方法。</p>
<p>RMSprop 和 Adadelta 都是為了解決 Adagrad 的學習率急劇下降的問題個別獨立開發出來的解決方式。RMSprop 實際上與 Adadelta 得出的第一個更新向量相同：</p>
<p style="text-align: center;"><span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t </sub>= 0.9<span>E[g</span><sup><sub>2</sub></sup><span>]</span><sub>t </sub> + 0.1<span>g</span><sup>2</sup><sub>t<br/></sub><span>θ</span><sub>t+1 </sub>= <span>θ</span><sub>t</sub> <span>−</span><span><span> </span></span><span>[η / (E[g<sup><sub>2</sub></sup><span>]</span><sub>t </sub></span><span>+ ϵ)</span><sup>½</sup><span>]</span><span>·g</span><sub>t</sub></p>
<p>RMSprop 也將學習率除以梯度平方的指數衰減平均值。Hinton 建議 γ 設為 0.9，好的預設學習率 η 數值為 0.001。</p>
<h4>Adam</h4>
<p>Adaptive Moment Estimation 自適應矩評估 (Adam) 是另一種計算每個評估學習率的方法。出了儲存過去梯度平 方的指數衰減平均值 <span>v</span><sub>t</sub>，就像 Adadelta 和 RMSprop 一樣，Adam 還保留過去梯度的指數衰減平均值 <span>m</span><sub>t</sub>，類似動量 (Momentum)。如果 Momentum 被視為順著斜坡下滑的球，而 Adam 則是像一個帶有摩擦的沉重的球，因此更適合待在 error face 平坦的最小值區域。計算過去梯度平方的衰減平均值 <span>m</span><sub>t</sub> 和 <span>v</span><sub>t</sub> 分別如下：</p>
<p style="text-align: center;">m<sub>t</sub> = <span>β</span><sub>1</sub><span>m</span><sub>t-1</sub> + (1 − <span>β</span><sub>1</sub>)<span>·g</span><sub>t<br/></sub><span>v</span><sub>t</sub> = <span>β</span><sub>2</sub><span>v</span><sub>t-1</sub> + (1 − <span>β</span><sub>2</sub>)<span>·</span><span>g</span><sup>2</sup><sub>t</sub></p>
<p><span>m</span><sub>t</sub> 和 <span>v</span><sub>t</sub> 分別是第一階矩平均估計值和第二階矩無中心方差估計值，因此是方法的名稱。像 <span>m</span><sub>t</sub> 和 <span>v</span><sub>t </sub>被初始化為向量 o，Adam 的作者觀察到它們偏向零，特別是在初始 time step，尤其是在衰減率較小的時候 (也就是說 <span>β</span><sub>1</sub><span><span> </span></span>和 <span>β</span><sub>2</sub><span><span> </span></span>趨近於 1) 藉由計算校正偏差第一矩 <em class="ph i">m̂</em><sub>t </sub>和第二矩 <em class="ph i">v̂<sub>t </sub></em>抵消偏差：</p>
<p style="text-align: center;"><em class="ph i">m̂</em><sub>t</sub> = <span>m</span><sub>t</sub><span><span> </span></span>/(1 − <span>β<sup>t</sup></span><sub>1</sub>)<br/><em class="ph i">v̂<sub>t</sub></em> = <span>v</span><sub>t</sub><span><span> </span></span>/(1 − <span>β<sup>t</sup></span><sub>2</sub>)</p>
<p>使用他們去更新參數，就像 Adadelta 和 RMSprop 中所看到的那樣，這將產生 Adam 更新規則：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − [η (<em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½ </sup></span></span>+ ϵ] <em class="ph i">m̂</em><span></span><sub>t</sub><span><span> </span></span></p>
<p>β<sub>1</sub> 預設值建議為 0.9，β<sub>2</sub> 預設值建議為 0.999，ϵ 預設值建議為 <span>10</span><sup>−8</sup>。根據經驗證明 Adam 表現良好，並且與其他自適應學習算法相比具有優勢。</p>
<h4>AdaMax</h4>
<p>在 Adam 更新規則中的 <span>v</span><sub>t</sub> 係數是與梯度成反比地縮放過去梯度的範數 (通過 <span>v</span><sub>t-1</sub> 項) 和當前梯度 <span>|</span><span>g</span><sub>t</sub><span>|</span><sup>2 </sup>：</p>
<p style="text-align: center;"><span>v</span><sub>t</sub><span><span> </span></span>= <span>β</span><sub>2</sub><span>v</span><sub>t-1</sub> + (1 − <span>β</span><sub>2</sub>)|<span>g</span><sub>t</sub>|<sup>2 </sup></p>
<p>我們轉換這個更新到 ℓp。注意 <span>β</span><sub>2</sub> 參數化為 <span>β<sup>p</sup></span><sub>2</sub>：</p>
<p style="text-align: center;"><span>v</span><sub>t</sub><span><span> </span></span>= <span>β<sup>p</sup></span><sub>2</sub><span>v</span><sub>t-1</sub> + (1 − <span>β<sup>p</sup></span><sub>2</sub>)<span>|</span><span>g</span><sub>t</sub><span>|</span><sup>p</sup></p>
<p>大規範 p 值使數值上變得不穩定，這就是為什麼 ℓ1 和 ℓ2 規範在實踐中是最常見的。然而，ℓ∞ 通常也表現出穩定 的行為。作者 (Kingma and Ba, 2015) 提出了 AdaMax 並證明了和 ℓ∞ 收斂到更穩定的值。為了避免與 Adam 混用，所以使用 u<sub>t</sub><span><span> </span></span>來表示無窮範數約束 <span>v</span><sub>t</sub><span><span> </span></span>：</p>
<p style="text-align: center;"><span>u</span><sub>t</sub> = <span>β<sup>∞</sup></span><sub>2</sub><span>v</span><sub>t-1</sub> + (1 − <span>β<sup>∞</sup></span><sub>2</sub>)|<span>g</span><sub>t</sub>|<sup>∞</sup> <br/>= max(<span>β</span><sub>2</sub> · <span>v</span><sub>t-1</sub>, |gt|)</p>
<p>替換為 Adam 更新公式 <span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½ </sup></span></span><span>+ ϵ </span>和 <span>u</span><sub>t</sub> 得出 AdaMax 更新規則：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − η<span>·</span><span>u</span><sub>t</sub><span>·</span><em class="ph i">m̂</em><sub>t</sub><span><span> </span></span></p>
<p>注意 u<sub>t</sub> 依靠最大運算，不建議 Adam 中的 <span>m</span><sub>t</sub><span><span> </span></span>和 <span>v</span><sub>t</sub><span><span> </span></span>偏向零，這就是為什麼不需要針對 <span>u</span><sub>t</sub> 計算偏差。好的預設值 η = 0.002，<span>β</span><sub>1 </sub>= 0.9 和 <span>β</span><sub>2</sub> = 0.999。</p>
<h4>Nadam</h4>
<p>Nadam (Nesterov-accelerated Adaptive Moment Estimation，Nesterov 加速的自適應矩估計)，結合 Adam 和 NAG。為了將 NAG 納入 Adam，需要修改動量項 <span>m</span><sub>t</sub>。使用先前符號回顧動量更新規則：</p>
<p style="text-align: center;"><span>g</span><sub>t</sub> = ∇<span>θ</span><sub>t </sub>J(<span>θ</span><sub>t</sub>) <br/><span>m</span><sub>t</sub><span><span> </span></span>= γ<span>m</span><sub>t-1</sub><span><span> </span></span> + η<span>g</span><sub>t</sub><br/><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t </sub>− <span>m</span><sub>t</sub></p>
<p>J是目標函數，γ 是動量衰減項，η 是 step size(學習率)，上面的第三個方程式擴展為：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − (γ<span>m</span><sub>t-1</sub> + η<span>g</span><sub>t</sub>)</p>
<p>再次證明了動量涉及在前一個動量向量的方向上往前一步和在當前梯度的方向上邁出一步。NAG 然後允許計算梯度之前透過更新動量步長參數使梯度方向上執行更精確的步長。因此，我們只需要修改梯度 <span>g</span><sub>t</sub> 到達 NAG：</p>
<p style="text-align: center;"><span>g</span><sub>t</sub> = ∇<span>θ</span><sub>t</sub> J(<span>θ</span><sub>t</sub> − γ<span>m</span><sub>t-1</sub>) <br/><span>m</span><sub>t</sub><span><span> </span></span>= γ<span>m</span><sub>t-1</sub> + η<span>g</span><sub>t</sub><br/><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − <span>m</span><sub>t</sub><span><span> </span></span></p>
<p>Dozat 建議修改 NAG：一次用於更新梯度 <span>g</span><sub>t</sub> 第二次更新參數 <span>θ</span><sub>t+1</sub>，直接應用先前的動量向量來更新當前參數：</p>
<p style="text-align: center;"><span>g</span><sub>t</sub> = ∇<span>θ</span><sub>t</sub> J(<span>θ</span><sub>t</sub>) <br/><span>m</span><sub>t</sub><span><span> </span></span>= γ<span>m</span><sub>t-1</sub> + η<span>g</span><sub>t</sub><br/><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − (γ<span>m</span><sub>t</sub> + η<span>g</span><sub>t</sub>)</p>
<p>為了將 Nesterov 動量添加到 Adam，可以類似地用當前動量向量替換以前的動量向量。回想一下 Adam 更新規則 如下：</p>
<p style="text-align: center;"><span>m</span><sub>t</sub><span><span> </span></span>= <span>β</span><sub>1</sub><span>m</span><sub>t-1</sub> + (1 − <span>β</span><sub>1</sub>)<span>g</span><sub>t</sub><br/><em class="ph i">m̂</em><sub>t</sub><span><span> </span></span> = <span>m</span><sub>t</sub><span><span> / (</span></span>1 − <span>β<sup>t</sup></span><sub>1</sub>)<br/><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − { η / [<span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½</sup></span></span> + ϵ ]} <em class="ph i">m̂</em><sub>t</sub><span><span> </span></span></p>
<p>用定義拓展第二個方程式：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub>  − <span>{ η</span><span> / [</span><span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½</sup></span></span><span><span> </span>+ ϵ ]}</span>{( <span>β</span><sub>1</sub><span>m</span><sub>t-1</sub>) / (1 − <span>β<sup>t</sup></span><sub>1</sub><span> )</span> + [(1 − <span>β</span><sub>1</sub>)<span>g</span><sub>t</sub>]/(1 − <span>β<sup>t</sup></span><sub>1</sub> )}</p>
<p>注意 (<span>β</span><sub>1</sub><span>m</span><sub>t-1</sub>) / (1−<span>β<sup>t</sup></span><sub>1</sub>)只是前一個的 time step 的動量向量的偏差來校正評估。因此，可以將其替換為 <em class="ph i">m̂</em><sub>t-1</sub>：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t </sub>− <span>{ η</span><span> / [</span><span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½</sup></span></span><span><span> </span>+ ϵ ]}[</span><span>β</span><sub>1<em class="ph i">m̂</em><sub>t-1</sub><span><span> </span></span></sub> + (1 − <span>β</span><sub>1</sub>)<span>g</span><sub>t</sub> / (1 − <span>β<sup>t</sup></span><sub>1</sub> )]</p>
<p>為簡化，因為無論如何將在下一步中替換分母，所以忽略了分母 1 − <span>β<sup>t</sup></span><sub>1</sub>。該方程式再次看起來和上面擴展的動量更 新規則非常相似。可以像以前一樣添加 Nesterov 動量，方法是用當前動量向量偏差校正後的評估值替換前一時間步 長的動量向量偏差校正後的評估值，這為我們提供了 Nadam 更新規則：</p>
<p style="text-align: center;"><span>θ</span><sub>t+1</sub> = <span>θ</span><sub>t</sub> − <span> </span><span>{ η</span><span> / [</span><span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½</sup></span></span><span><span> </span>+ ϵ ]}</span>[(<span>β</span><sub>1<em class="ph i">m̂</em><sub>t</sub><span><span> </span></span></sub> + (1 − <span>β</span><sub>1</sub>)<span>g</span><sub>t</sub> / (1 − <span>β<sup>t</sup></span><sub>1</sub> )]</p>
<h4>AMSGrad</h4>
<p>Reddi 等（2018）。正式化了這個問題，並指出了泛化行為不佳的原因：將過去梯度平方的指數移動平均值作為自適應學習率方法。雖然引入指數平均值的動機很充分：應防止學習率隨著訓練的進行而變得無限小；但這也是 Adagrad 算法的關鍵缺陷。在其他情況下，短期記憶的梯度成為障礙。</p>
<p>在 Adam 收斂到次優解的環境中，已經觀察到一些小型批次提供了較大且信息豐富的梯度，但是這些小型批次很少出現，因此指數平均會減小其影響，從而導致收斂性較差。作者 (資料來源的作者) 提供了一個簡單的凸型優化問題的例子，其中 Adam 可以觀察到相同的行為。AMSGrad算法是為了解決此問題，這算法使用了過去梯度平方的最大值 <span>v</span><sub>t </sub>而不是指數平均值來更新參數。<span>v</span><sub>t </sub>的定義與先前的 Adam 相同：</p>
<p style="text-align: center;"><span>v</span><sub>t </sub>= <span>β</span><sub>2</sub><span>v</span><sub>t-1 </sub>+ (1 − <span>β</span><sub>2</sub>)<span>g</span><sup>2</sup><sub>t</sub></p>
<p>而不是直接使用 vt(或其偏差更正的版本 vˆt），如果現在使用以前值的大於現在的值：</p>
<p style="text-align: center;"><em class="ph i">v̂<sub>t</sub></em> = max(<em class="ph i">v̂<sub>t-1 </sub></em>, <span>v</span><sub>t</sub>)</p>
<p>這方式 AMSGrad 不會增加步長 (step size)，從而避免了 Adam 遇到的問題。為了簡化，AMSGrad 去除了 Adam 的去偏差 (debias) 步驟。可以看到完整的 AMSGrad 更新，沒有經過偏差校正的估計：</p>
<p style="text-align: center;"><span>m</span><sub>t</sub><span><span> </span></span>= <span>β</span><sub>1</sub><span>m</span><sub>t-1</sub><span><span> </span></span>+ (1 − <span>β</span><sub>1</sub>)<span>g</span><sub>t</sub><br/><span>v</span><sub>t </sub>= <span>β</span><sub>2</sub><span>v</span><sub>t-1</sub> + (1 − <span>β</span><sub>2</sub>)<span>g</span><sup>2</sup><sub>t</sub><br/>vˆt = max(<em class="ph i">v̂<sub>t-1 </sub></em>, <span>v</span><sub>t </sub>) <br/><span>θ</span><sub>t+1</sub> = θt − <span>{ η / [</span><span>(</span><em class="ph i">v̂<sub>t </sub></em><span><span><span>)</span><sup>½</sup></span></span><span><span> </span>+ ϵ ]}</span> <span>m</span><sub>t</sub><span><span> </span></span></p>
<p>在小型數據集和 CIFAR-10 上，與 Adam 相比，性能有所提高。但是，其他實驗顯示其性能與 Adam 相近或更 差。在實際運用，AMSGrad 是否能勝過 Adam，還有待觀察。</p>
<h4>Gradient noise</h4>
<p>增加 noise 跟隨高斯分布 N(0, σ<sup>2</sup><sub>t</sub> ) 對每個梯度更新：</p>
<p style="text-align: center;">g<sub>t,i</sub> = <span>g</span><sub>t,i</sub> + N(0,<span> σ</span><sup>2</sup><span></span><sub>t</sub> )</p>
<p>根據排定時間對差異計算：</p>
<p style="text-align: center;"><span><span> </span>σ</span><sup>2</sup><span></span><sub>t</sub> = η /(1 + t)<sup>γ</sup></p>
<p>添加這種 noise 對不良初始化的網絡可使其強化，並有助於訓練特別深且復雜的網絡。他們懷疑增加的噪聲使模型 有更多的機會逃脫並找到新的局部極小值，這對於更深的模型而言更常見。</p>
<br />Stochastic gradient descent << <a href='Stochastic gradient descent.html'>Previous</a> <a href='程式.html'>Next</a> >> 程式</div>
        
    <!-- footer -->
      <div class="container">
        <div class="row pt-3 mx-auto">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="icon-heart" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
        </div>
      </div>
    <!-- for footer -->
    
        </div> <!-- for site wrap -->
            <!-- <script src="../cmsimde/static/chimper/js/jquery-3.3.1.min.js"></script> -->
            <script src="../cmsimde/static/chimper/js/jquery-migrate-3.0.1.min.js"></script>
            <script src="../cmsimde/static/chimper/js/jquery-ui.js"></script>
            <script src="../cmsimde/static/chimper/js/popper.min.js"></script>
            <script src="../cmsimde/static/chimper/js/bootstrap.min.js"></script>
            <script src="../cmsimde/static/chimper/js/owl.carousel.min.js"></script>
            <script src="../cmsimde/static/chimper/js/jquery.stellar.min.js"></script>
            <script src="../cmsimde/static/chimper/js/jquery.countdown.min.js"></script>
            <script src="../cmsimde/static/chimper/js/jquery.magnific-popup.min.js"></script>
            <script src="../cmsimde/static/chimper/js/bootstrap-datepicker.min.js"></script>
            <script src="../cmsimde/static/chimper/js/aos.js"></script>
            <!--
            <script src="../cmsimde/static/chimper/js/typed.js"></script>
                    <script>
                    var typed = new Typed('.typed-words', {
                    strings: ["Web Apps"," WordPress"," Mobile Apps"],
                    typeSpeed: 80,
                    backSpeed: 80,
                    backDelay: 4000,
                    startDelay: 1000,
                    loop: true,
                    showCursor: true
                    });
                    </script>
            -->
            <script src="../cmsimde/static/chimper/js/main.js"></script>
        
<!-- 啟用 LaTeX equations 編輯 -->
  <!-- <script>
  MathJax = {
    tex: {inlineMath: [['$', '$'], ['\(', '\)']]}
  };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>-->
    </body></html>
        