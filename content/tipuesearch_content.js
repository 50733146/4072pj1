var tipuesearch = {"pages": [{'title': 'About', 'text': '專題題目:\xa0 \n 強化學習在機電系統中之應用 \n Application of reinforcement learning in mechatronic systems \n 討論區:\xa0 https://github.com/mdecourse/4072pj1/discussions \n PDF: 4072pj1.pdf \n 組員: \n 40723110 \n 40723115 \n 40723138 \n 40723148 \n 40723150 \n \n \n', 'tags': '', 'url': 'About.html'}, {'title': '專題定位', 'text': '2021/01/08 \n Adagrad:  2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf \n 2012_ADADELTA AN ADAPTIVE LEARNING RATE METHOD.pdf \n Adam:  2015_ADAM A METHOD FORSTOCHASTICOPTIMIZATION.pdf \n 2017_An overview of gradient descent optimizationalgorithms.pdf \n 2018_A Comparative Analysis of Gradient Descent-Based Optimization Algorithms on Convolutional Neural Networks.pdf \n 第一組的理論與程式應用到第三組平面機構合成: \n 2015_Dimensional synthesis of mechanical linkages usingartificial neural networks and Fourier descriptors.pdf \n 2017_Four-Bar Linkage Synthesis Using Non-ConvexOptimization.pdf \n 2018_Synthesis of a novel planar linkage to visit up to eight poses.pdf \n \n 利用  Webots  與\xa0 https://github.com/mdecourse/deepbots, \xa0透過  https://github.com/mdecourse/deepbots-tutorials \xa0與  https://github.com/mdecourse/deepworlds \xa0進一步了解 deepbots 如何應用在 Webots 中的受控系統. \n 專題目的在探討與  http://mde.tw/airhockey \xa0( https://github.com/mdecourse/airhockey  )中虛實整合系統設計與控制有關的機器學習應用. \n 下載  Webots_R2020brev1_portable.7z \n 參考資料: \n https://github.com/mdecourse/RL_Webots \xa0 \n Deep Reinforcement Learning.pdf \xa0(2019) \n 2020_Book_DeepReinforcementLearning.pdf \n 2019_Learning to Walk via Deep Reinforcement Learning \n 2016_3d_simulated_robot_manipulation_using_deep_reinforcement_learning.pdf \n 2002_Book_HandbookOfMarkovDecisionProces.pdf \n Deepbots: A Webots-Based Deep Reinforcement Learning Framework for Robotics \n', 'tags': '', 'url': '專題定位.html'}, {'title': 'LaTeX', 'text': 'https://github.com/sppmg/TW_Thesis_Template/wiki/無腦手冊 \n https://github.com/wengan-li/ncku-thesis-template-latex \n https://github.com/HW-Lee/nthu-thesis-template \n     Features & Benefits\n    Templates\n    Plans & Pricing\n    Help\n    Register\n    Log In\n\nNTUST Thesis template 1.7.1 (Chinese Version)\nAuthor\nDing-Jie Huang, Chien-Chun Ni\nLicense\nCreative Commons CC BY 4.0\nAbstract\n\nThis is the Chinese Version of NTUST thesis template v1.7.1, please check "NTUST Thesis template Overleaf English Version" if you need the English one.\n\n本範本是為台灣科技大學同學們所編寫的碩博士論文Latex模板， 主要由元智大學碩博士論文latex範本改編而來， 期望加快各位同學撰寫論文的速度。\n\nP.S 原始陳念波老師的元智大學論文範本為 http://exciton.eo.yzu.edu.tw/~lab/latex/latex_note.html\n\nSupport 14pt font for this version\nTags\nFind More Templates\nNTUST Thesis template 1.7.1 (Chinese Version)\n\n    © 2020 OverleafPrivacy and TermsSecurityContact UsAboutBlog\n\n    Overleaf on TwitterOverleaf on FacebookOverleaf on LinkedIn\n\nSource\n\n% this file is encoded in utf-8\n% v1.7\n\n\\documentclass[12pt, a4paper]{ntust_report} \n\n\\usepackage{fontspec}   %加這個就可以設定字體 \n\\usepackage{xeCJK}       %讓中英文字體分開設置 \n\n%設定主要字型，也就是英文字型\n\\setmainfont[Mapping=tex-text]{Times New Roman}            \n\n%設定中文字型\n%參考 https://www.overleaf.com/learn/latex/Questions/What_OTF/TTF_fonts_are_supported_via_fontspec%3F#Chinese\n\\setCJKmainfont{cwTeXKai}      \n\n\\XeTeXlinebreaklocale "zh"                %這兩行一定要加，中文才能自動換行 \n\\XeTeXlinebreakskip = 0pt plus 1pt       %這兩行一定要加，中文才能自動換行\n\n\\input{common_env}  %基本的環境設定  無需改變  \n\n\\begin{document}\n\n\n\t\\input{chinese_trans} %在此檔案處定義文章中的中文名詞\n\n\t%----------------------------------------------------------------------------------------------------------------------------------------------------------\n\t%%% 以下是載入前頁、本文、後頁\n\t% 此行請勿更動\n\n\t%----------------------------------------------------------------------------------------------------------------------------------------------------------\n\t% front matter 前頁\n\t% 包括封面、書名頁、中文摘要、英文摘要、誌謝、目錄、表目錄、圖目錄、符號說明\n\t% 在撰寫各章草稿時，可以把此部份「關掉」，以節省無謂的編譯時間。\n\t% 實際內容由\n\t%    my_names.tex, my_cabstract.tex, my_eabstract.tex, my_ackn.tex, my_symbols.tex\n\t% 決定\n\t% ntust_frontpages.tex 此檔只提供整體架構的定義，不需更動\n\t% 在撰寫各章草稿時，可以把此部份「關掉」，以節省無謂的編譯時間。\n\t\n\t\\input{frontpages/ntust_frontpages.tex} \n\n\n\n\n\t%----------------------------------------------------------------------------------------------------------------------------------------------------------\n\t% main body 論文主體。建議以「章」為檔案分割的依據。\n\t% 以下為建議的命名分類\n\t%   introduction.tex   related_work.tex  protocol.tex  evaluation.tex  conclusion.tex\n\t% 做為這幾個「章」的檔案名稱，並將檔案存放於資料夾 sections/ 下\n\t% 實際命名方式可以隨你意\n\t% 在撰寫各章草稿時，可以把其他章節關掉 (行首加百分號)\n\t%\\input{example/example_body.tex}  % 所附的範例\n\n\t\\input{sections/introduction.tex}\n\t\\input{sections/relative-work.tex}\n\t\\input{sections/method.tex}\n\t\\input{sections/Design.tex}\n\t\\input{sections/evaluation.tex}\n\t\\input{sections/Conclusion.tex}\n\n\t%----------------------------------------------------------------------------------------------------------------------------------------------------------\n\t% back pages 後頁\n\t% 包括參考文獻、附錄、自傳\n\t% 實際內容由\n\t%    my_bib.bib, my_appendix.tex, my_vita.tex\n\t% 決定\n\t% ntust_backpages.tex 此檔只提供整體架構的定義，不需更動\n\t% 在撰寫各章草稿時，可以把此部份「關掉」，以節省無謂的編譯時間。\n\t%\\bibliographystyle{unsrt} \n\t\\input{backpages/ntust_backpages.tex}\n\t%\\bibliographystyle{unsrt} \n\n\n\n\\end{document} \n \n\n \n \n', 'tags': '', 'url': 'LaTeX.html'}, {'title': '參考資料', 'text': '\n https://easyai.tech/en/blog/reinforcement-learning-with-python/ \n https://github.com/openai/gym \n https://github.com/bhyang/gym-vrep \n https://arxiv.org/pdf/1608.05742.pdf \n https://github.com/stepjam/PyRep \n https://arxiv.org/abs/1906.11176 \n https://github.com/ycps/vrep-env \n https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4 \n https://github.com/araffin/learning-to-drive-in-5-minutes \n https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ \n https://upcommons.upc.edu/bitstream/handle/2117/133279/tfm-alex-cabaneros.pdf \n \n', 'tags': '', 'url': '參考資料.html'}, {'title': '研究生考試', 'text': 'csme 研究生考試科目與題型:  考試內容 及 考古題 \n', 'tags': '', 'url': '研究生考試.html'}, {'title': '動態網站', 'text': '設定步驟請參考: \n https://github.com/mdecourse/project2020-1/issues/4 \n CMSiMDE 執行所需模組 \n sudo pip3 install flask flask_cors bs4 lxml \n uwsgi 所需模組 \n sudo apt install uwsgi uwsgi-plugin-python3 \n sudo pip3 install uwsgi \n /etc/nginx/sites-available/default 附加 server 設定  \n server {\n \n    listen 9443 ssl;\n    listen [::]:9443 ssl;\n \n    # 指定 static 所在位置\n    location /static {\n\talias /home/yen/cad1_site/cmsimde/static/;\n    }\n \n    location / {\n\t# 導入 uwsgi_params 設定參數\n\tinclude uwsgi_params;\n\t# 根目錄設為近端的 8080 port \n\tuwsgi_pass  127.0.0.1:8080;\n    }\n \n    ssl_certificate /home/yen/localhost.crt;\n    ssl_certificate_key /home/yen/localhost.key;\n    #ssl_certificate /etc/letsencrypt/live/cad1.kmol.info/fullchain.pem;\n    #ssl_certificate_key /etc/letsencrypt/live/cad1.kmol.info/privkey.pem;\n    ssl_session_timeout 5m;\n    ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers "HIGH:!aNULL:!MD5 or HIGH:!aNULL:!MD5:!3DES";\n    ssl_prefer_server_ciphers on;\n    try_files $uri $uri/ =404;\n} \n 建立 self-signed key sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout localhost.key -out localhost.crt /home/yen/uwsgi_ini/uwsgi.ini \n [uwsgi]\nsocket = :8080\nuid = yen\ngid = yen\nplugins-dir = /usr/lib/uwsgi/plugins/\nplugin = python3\nmaster = true\nprocess = 4\nthreads = 2\nchdir = /home/yen/cad1_site/cmsimde\nwsgi-file = /home/yen/cad1_site/cmsimde/wsgi.py \n uwsgi emperor 手動測試 \n /usr/bin/uwsgi --emperor /home/yen/uwsgi_ini 防火牆設定 \n 先暫時關閉 ufw \n ufw disable \n 允許設計系 IP v6 網段連線 9443 port \n ufw allow from 2001:288:6004:17::/32 to any port 9443 \n 其他網段主機一律]不准連線 \n ufw deny 9443 \n 重新開啟 ufw 防火牆 \n ufw enable \n /etc/systemd/system 目錄中建立 cmsimde.service 檔案 \n \n [Unit]\nDescription=uWSGI to serve CMSiMDE \nAfter=network.target\n \n[Service]\nUser=yen\nGroup=yen\nWorkingDirectory=/home/yen/uwsgi_ini\nExecStart=/usr/bin/uwsgi --emperor /home/yen/uwsgi_ini\n \n[Install]\nWantedBy=multi-user.target \n \n 接著將 cmsimde 服務設為隨系統開機啟動: \n sudo systemctl enable cmsimde\n \n 若要取消 cmsimde 服務隨系統開機啟動: \n sudo systemctl disable cmsimde\n \n 手動啟動 cmsimde.service 服務 \n sudo systemctl start cmsimde\n \n 手動停止 cmsimde.service 服務 \n sudo systemctl stop cmsimde \n \n', 'tags': '', 'url': '動態網站.html'}, {'title': '數位簽章', 'text': 'https://letsencrypt.org/ \n https://certbot.eff.org/lets-encrypt/ubuntufocal-nginx \xa0 \n sudo apt-get update\nsudo apt-get install software-properties-common\nsudo add-apt-repository universe\nsudo apt-get update\n\nsudo apt-get install certbot python3-certbot-nginx\n\nsudo certbot certonly --nginx\n\nsudo certbot renew --dry-run \n \n', 'tags': '', 'url': '數位簽章.html'}, {'title': '機電控制', 'text': 'Mechatronic system control \n 控制卡: \n Arduino \n Raspberry Pi \n 機電系統: \n iRobot Create \n', 'tags': '', 'url': '機電控制.html'}, {'title': 'iRobot Create', 'text': 'https://www.cyberbotics.com/doc/guide/create \n \n https://github.com/mgobryan/pycreate \xa0 \n https://github.com/mdecourse/create_autonomy \xa0 \n iRobot Create Manual.pdf \n iRobot Create cookbook.pdf \n iRobot Create Command Module manual v2.pdf \n iRobot Create Open Interface v2.pdf \n Use Arduino to control iRobot Create.pdf \n Use Arduino to control iRobot Create_2.pdf \n', 'tags': '', 'url': 'iRobot Create.html'}, {'title': '深度學習', 'text': 'Solving Nonlinear andHigh-Dimensional PartialDifferential Equations viaDeep Learning.pdf \n https://github.com/alialaradi/DeepGalerkinMethod \n DGM A deep learning algorithm for solving partial differentialequations.pdf \n \n https://cloud4scieng.org/ \xa0雲端運算 \n https://www.uio.no/studier/emner/matnat/ifi/IN5400/v20/material/week1/ \n https://wcm.kmol.info:8443  (KMOLer only) \n https://github.com/mdecourse/cd2020pj1 \n https://cs.stanford.edu/people/karpathy/convnetjs/\xa0 \n http://neuralnetworksanddeeplearning.com/chap1.html \xa0 \n https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html \n http://deeplearning.net/ \n Playing Atari with Deep Reinforcement Learning.pdf \n Robust Auto-parking: Reinforcement Learning based Real-time Planning Approach with Domain Template.pdf \n Automatic Car Parking: A Reinforcement Learning Approach.pdf \n Real-time image-based parking occupancy detection using deep learning.pdf \n \n \n 以下為 convnet javascript 應用 \n  import convnetjs library  \n \n  javascript goes here  \n \n \n \n', 'tags': '', 'url': '深度學習.html'}, {'title': '電腦', 'text': '專題可攜 22GB.7z \n 電腦輔助設計室 2016 更換的 電腦硬體 : \n 2016-2 協同設計室 6 台電腦 \n Intel Core i7-6700 3.4 GHz Ram 16GB \n 顯示卡 NVIDIA GeForce GTX 950 2GB 768 CUDA cores version 10.1 \n 安裝 Pytorch 指令 : \n pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f  https://download.pytorch.org/whl/torch_stable.html https://github.com/pytorch/examples Deep learning with python.pdf Deep Learning with Pytorch.pdf  ( 來源 ) \n', 'tags': '', 'url': '電腦.html'}, {'title': 'Flask', 'text': 'Flutter 前端與後端 Flask + Keras 結合應用 \n https://medium.com/analytics-vidhya/deploy-deep-learning-models-as-rest-apis-using-keras-and-access-from-a-flutter-app-9a6752f0d907 \n \n Flutter send image file to backend: \n \n import \'package:flutter/material.dart\';\nimport \'dart:io\';\nimport \'package:http/http.dart\' as http;\nimport \'package:image_picker/image_picker.dart\';\nimport \'package:mime/mime.dart\';\nimport \'dart:convert\';\nimport \'package:http_parser/http_parser.dart\';\nimport \'package:toast/toast.dart\';\n\nvoid main() => runApp(MyApp());\n\nclass MyApp extends StatelessWidget {\n  // This widget is the root of your application.\n  @override\n  Widget build(BuildContext context) {\n    return MaterialApp(\n        title: \'Image Upload Demo\',\n        theme: ThemeData(primarySwatch: Colors.pink),\n        home: ImageInput());\n  }\n}\n\nclass ImageInput extends StatefulWidget {\n  @override\n  State<StatefulWidget> createState() {\n    return _ImageInput();\n  }\n}\n\nclass _ImageInput extends State<ImageInput> {\n  // To store the file provided by the image_picker\n  File _imageFile;\n\n  // To track the file uploading state\n  bool _isUploading = false;\n\n  String baseUrl = \'http://YOUR_IPV4_ADDRESS/flutterdemoapi/api.php\';\n\n  void _getImage(BuildContext context, ImageSource source) async {\n    File image = await ImagePicker.pickImage(source: source);\n\n    setState(() {\n      _imageFile = image;\n    });\n\n    // Closes the bottom sheet\n    Navigator.pop(context);\n  }\n\n  Future<Map<String, dynamic>> _uploadImage(File image) async {\n    setState(() {\n      _isUploading = true;\n    });\n\n    // Find the mime type of the selected file by looking at the header bytes of the file\n    final mimeTypeData =\n        lookupMimeType(image.path, headerBytes: [0xFF, 0xD8]).split(\'/\');\n\n    // Intilize the multipart request\n    final imageUploadRequest =\n        http.MultipartRequest(\'POST\', Uri.parse(baseUrl));\n\n    // Attach the file in the request\n    final file = await http.MultipartFile.fromPath(\'image\', image.path,\n        contentType: MediaType(mimeTypeData[0], mimeTypeData[1]));\n\n    // Explicitly pass the extension of the image with request body\n    // Since image_picker has some bugs due which it mixes up\n    // image extension with file name like this filenamejpge\n    // Which creates some problem at the server side to manage\n    // or verify the file extension\n    imageUploadRequest.fields[\'ext\'] = mimeTypeData[1];\n\n    imageUploadRequest.files.add(file);\n\n    try {\n      final streamedResponse = await imageUploadRequest.send();\n\n      final response = await http.Response.fromStream(streamedResponse);\n\n      if (response.statusCode != 200) {\n        return null;\n      }\n\n      final Map<String, dynamic> responseData = json.decode(response.body);\n\n      _resetState();\n\n      return responseData;\n    } catch (e) {\n      print(e);\n      return null;\n    }\n  }\n\n  void _startUploading() async {\n    final Map<String, dynamic> response = await _uploadImage(_imageFile);\n    print(response);\n    // Check if any error occured\n    if (response == null || response.containsKey("error")) {\n      Toast.show("Image Upload Failed!!!", context,\n          duration: Toast.LENGTH_LONG, gravity: Toast.BOTTOM);\n    } else {\n      Toast.show("Image Uploaded Successfully!!!", context,\n          duration: Toast.LENGTH_LONG, gravity: Toast.BOTTOM);\n    }\n  }\n\n  void _resetState() {\n    setState(() {\n      _isUploading = false;\n      _imageFile = null;\n    });\n  }\n\n  void _openImagePickerModal(BuildContext context) {\n    final flatButtonColor = Theme.of(context).primaryColor;\n    print(\'Image Picker Modal Called\');\n    showModalBottomSheet(\n        context: context,\n        builder: (BuildContext context) {\n          return Container(\n            height: 150.0,\n            padding: EdgeInsets.all(10.0),\n            child: Column(\n              children: <Widget>[\n                Text(\n                  \'Pick an image\',\n                  style: TextStyle(fontWeight: FontWeight.bold),\n                ),\n                SizedBox(\n                  height: 10.0,\n                ),\n                FlatButton(\n                  textColor: flatButtonColor,\n                  child: Text(\'Use Camera\'),\n                  onPressed: () {\n                    _getImage(context, ImageSource.camera);\n                  },\n                ),\n                FlatButton(\n                  textColor: flatButtonColor,\n                  child: Text(\'Use Gallery\'),\n                  onPressed: () {\n                    _getImage(context, ImageSource.gallery);\n                  },\n                ),\n              ],\n            ),\n          );\n        });\n  }\n\n  Widget _buildUploadBtn() {\n    Widget btnWidget = Container();\n\n    if (_isUploading) {\n      // File is being uploaded then show a progress indicator\n      btnWidget = Container(\n          margin: EdgeInsets.only(top: 10.0),\n          child: CircularProgressIndicator());\n    } else if (!_isUploading && _imageFile != null) {\n      // If image is picked by the user then show a upload btn\n\n      btnWidget = Container(\n        margin: EdgeInsets.only(top: 10.0),\n        child: RaisedButton(\n          child: Text(\'Upload\'),\n          onPressed: () {\n            _startUploading();\n          },\n          color: Colors.pinkAccent,\n          textColor: Colors.white,\n        ),\n      );\n    }\n\n    return btnWidget;\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return Scaffold(\n      appBar: AppBar(\n        title: Text(\'Image Upload Demo\'),\n      ),\n      body: Column(\n        children: <Widget>[\n          Padding(\n            padding: const EdgeInsets.only(top: 40.0, left: 10.0, right: 10.0),\n            child: OutlineButton(\n              onPressed: () => _openImagePickerModal(context),\n              borderSide:\n                  BorderSide(color: Theme.of(context).accentColor, width: 1.0),\n              child: Row(\n                mainAxisAlignment: MainAxisAlignment.center,\n                children: <Widget>[\n                  Icon(Icons.camera_alt),\n                  SizedBox(\n                    width: 5.0,\n                  ),\n                  Text(\'Add Image\'),\n                ],\n              ),\n            ),\n          ),\n          _imageFile == null\n              ? Text(\'Please pick an image\')\n              : Image.file(\n                  _imageFile,\n                  fit: BoxFit.cover,\n                  height: 300.0,\n                  alignment: Alignment.topCenter,\n                  width: MediaQuery.of(context).size.width,\n                ),\n          _buildUploadBtn(),\n        ],\n      ),\n    );\n  }\n} \n Server 端, 必須將 php 改為 Flask: \n     <?php\n    if(isset($_FILES["image"]["name"])) {\n      \n        // Make sure you have created this directory already\n        $target_dir = "uploads/";\n      \n        // Generate a random name \n        $target_file = $target_dir . md5(time()) . \'.\' . $_POST[\'ext\'];\n        $check = getimagesize($_FILES["image"]["tmp_name"]);\n        if($check !== false) {\n            if (move_uploaded_file($_FILES["image"]["tmp_name"], $target_file)) {\n          echo json_encode([\'response\' => "The image has been uploaded."]);\n           }else {\n          echo json_encode(["error" => "Sorry, there was an error uploading your file."]); \n        }\n        } else {\n            echo json_encode(["error" => "File is not an image."]);\n           \n        }\n    }\n     else {\n         echo json_encode(["error" => "Please provide a image to upload"]);\n    }\n    ?> \n \n \n', 'tags': '', 'url': 'Flask.html'}, {'title': '強化學習', 'text': 'https://github.com/dennybritz/reinforcement-learning \n http://incompleteideas.net/book/RLbook2018.pdf \n Artificial Intelligence (  人工智慧系列課程 ) \n \n Machine Learning ( 機器學習系列課程 ) \n \n Reinforcement Learning ( 強化學習系列課程 ) \n Learn to make good sequences of decision. \n \n https://github.com/keras-rl/keras-rl \n https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0 \n https://github.com/astooke/rlpyt \xa0( Document ) ( Blog ) \n https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch \n https://openai.com/blog/openai-baselines-ppo/ \xa0 \n Deep Q Learning \n \n Vanilla Policy Gradient Method \n Trust Region / Natural Policy Gradient Methods \n Proximal Policy Optimization Algorithms.pdf \xa0( 近端策略優化原理) \n https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html \n https://www.mlq.ai/deep-reinforcement-learning-pytorch-implementation/ \n', 'tags': '', 'url': '強化學習.html'}, {'title': '2021', 'text': 'need Python and Node.js and Typescript \n https://github.com/Positronic-IO/air-hockey-training-environment \xa0 \n https://github.com/Positronic-IO/air-hockey-web-ui \xa0 \n https://jonathan-hui.medium.com/rl-dqn-deep-q-network-e207751f7ae4 \xa0 \n https://keon.github.io/ \xa0 \n', 'tags': '', 'url': '2021.html'}, {'title': 'Reference', 'text': '2018_Robotic Harvesting of Fruiting Vegetables-A Simulation Approach in V-REP, ROS and MATLAB.pdf \n 2019_Hovering Control of a Quadrotor.pdf \n 2019_Accelerating Training of Deep Reinforcement Learning-based Autonomous Driving Agents Through Comparative Study of Agent and Environment Designs.pdf \n 2018_Curved Path Following with Deep Reinforcement Learning-Results from Three Vessel Models.pdf \n 2019_Application of deep reinforcement learning for control problems.pdf \n 2018_Path Following in Simulated Environments using the A3C Reinforcement Learning Method.pdf \n FlashRL_A Reinforcement Learning Platform.pdf \n', 'tags': '', 'url': 'Reference.html'}, {'title': '類神經網路學習', 'text': 'nn_and_air_hockey.7z \n 將  https://github.com/Purusharth07/Ping-Pong-Neural-Game- \xa0 改為  tensorflow 2.0  版本, 使用 Pygame 模擬. \n', 'tags': '', 'url': '類神經網路學習.html'}, {'title': 'neural_network_in_python.pdf', 'text': '說明前三章的程式碼 \n 2LayerNeuralNetwork.py \n origin codes: \n # 2 Layer Neural Network in NumPy\nimport numpy as np\n# X = input of our 3 input XOR gate\n# set up the inputs of the neural network (right from the table)\nX = np.array(([0,0,0],[0,0,1],[0,1,0], [0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]), dtype=float)\n# y = our output of our neural network\ny = np.array(([1], [0], [0], [0], [0], [0], [0], [1]), dtype=float)\n# what value we want to predict\nxPredicted = np.array(([0,0,1]), dtype=float)\nX = X/np.amax(X, axis=0) # maximum of X input array\n# maximum of xPredicted (our input data for the prediction)\nxPredicted = xPredicted/np.amax(xPredicted, axis=0)\n# set up our Loss file for graphing\nlossFile = open("SumSquaredLossList.csv", "w")\nclass Neural_Network (object):\n    def __init__(self):\n        #parameters\n        self.inputLayerSize = 3 # X1,X2,X3\n        self.outputLayerSize = 1 # Y1\n        self.hiddenLayerSize = 4 # Size of the hidden layer\n        # build weights of each layer\n        # set to random values\n        # look at the interconnection diagram to make sense of this\n        # 3x4 matrix for input to hidden\n        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n        # 4x1 matrix for hidden layer to output\n        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n    def feedForward(self, X):\n        # feedForward propagation through our network\n        # dot product of X (input) and first set of 3x4 weights\n        self.z = np.dot(X, self.W1)\n        # the activationSigmoid activation function - neural magic\n        self.z2 = self.activationSigmoid(self.z)\n        # dot product of hidden layer (z2) and second set of 4x1 weights\n        self.z3 = np.dot(self.z2, self.W2)\n        # final activation function - more neural magic\n        o = self.activationSigmoid(self.z3)\n        return o\n     def backwardPropagate(self, X, y, o):\n        # backward propagate through the network\n        # calculate the error in output\n        self.o_error = y - o\n        # apply derivative of activationSigmoid to error\n        self.o_delta = self.o_error*self.activationSigmoidPrime(o)\n        # z2 error: how much our hidden layer weights contributed to output\n        # error\n        self.z2_error = self.o_delta.dot(self.W2.T)\n        # applying derivative of activationSigmoid to z2 error\n        self.z2_delta = self.z2_error*self.activationSigmoidPrime(self.z2)\n        # adjusting first set (inputLayer --> hiddenLayer) weights\n        self.W1 += X.T.dot(self.z2_delta)\n        # adjusting second set (hiddenLayer --> outputLayer) weights\n        self.W2 += self.z2.T.dot(self.o_delta)\n    def trainNetwork(self, X, y):\n        # feed forward the loop\n        o = self.feedForward(X)\n        # and then back propagate the values (feedback)\n        self.backwardPropagate(X, y, o)\n    def activationSigmoid(self, s):\n        # activation function\n        # simple activationSigmoid curve as in the book\n        return 1/(1+np.exp(-s))\n    def activationSigmoidPrime(self, s):\n        # First derivative of activationSigmoid\n        # calculus time!\n        return s * (1 - s)\n    def saveSumSquaredLossList(self,i,error):\n        lossFile.write(str(i)+","+str(error.tolist())+\'\\n\')\n    def saveWeights(self):\n        # save this in order to reproduce our cool network\n        np.savetxt("weightsLayer1.txt", self.W1, fmt="%s")\n        np.savetxt("weightsLayer2.txt", self.W2, fmt="%s")\n    def predictOutput(self):\n        print ("Predicted XOR output data based on trained weights: ")\n        print ("Expected (X1-X3): \\n" + str(xPredicted))\n        print ("Output (Y1): \\n" + str(self.feedForward(xPredicted)))\n\nmyNeuralNetwork = Neural_Network()\ntrainingEpochs = 1000\n#trainingEpochs = 100000\n\nfor i in range(trainingEpochs): # train myNeuralNetwork 1,000 times\n    print ("Epoch # " + str(i) + "\\n")\n    print ("Network Input : \\n" + str(X))\n    print ("Expected Output of XOR Gate Neural Network: \\n" + str(y))\n    print ("Actual Output from XOR Gate Neural Network: \\n" + \\\n    str(myNeuralNetwork.feedForward(X)))\n    # mean sum squared loss\n    Loss = np.mean(np.square(y - myNeuralNetwork.feedForward(X)))\n    myNeuralNetwork.saveSumSquaredLossList(i,Loss)\n    print ("Sum Squared Loss: \\n" + str(Loss))\n    print ("\\n")\n    myNeuralNetwork.trainNetwork(X, y)\n\nmyNeuralNetwork.saveWeights()\nmyNeuralNetwork.predictOutput()p \n 定義input(X)和output(Y) \n X = np.array(([0,0,0],[0,0,1],[0,1,0], [0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]), dtype=float)\ny = np.array(([1], [0], [0], [0], [0], [0], [0], [1]), dtype=float) \n 設定神經元及權重 \n python\n def __init__(self):\n        # X1,X2,X3(自訂義input的神經元數量)\n        self.inputLayerSize = 3\n        # Y1(自訂義output的神經元數量)   \n        self.outputLayerSize = 1\n        # Size of the hidden layer(自訂義hiddenLayer的神經元數量)          \n        self.hiddenLayerSize = 4 \n        \n        # 設定第一層權重為隨機數值，input--->hidden\n        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n        \n        # 設定第二層權重為隨機數值，hidden--->output\n        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n \n feedForward(前饋) \n \n (圖片來源) \n  def feedForward(self, X):\n        # 第一層的動態方程式(activation function)輸入(z)\n        # z(activation function) = 第一層所有神經元的 input * weights 總和輸入到第二層的其中一個神經元\n        self.z = np.dot(X, self.W1)\n\n        # 第一層的動態方程式(activation function)輸出(a)\n        # z2(a) = 動態方程式(activation function)用Sigmoid function算法\n        self.z2 = self.activationSigmoid(self.z)\n\n        # 第二層的動態方程式(activation function)輸入(z)\n        # z3(activation function) = 第二層所有神經元的 input * weights 總和輸入到輸出層的(其中一個)神經元\n        self.z3 = np.dot(self.z2, self.W2)\n\n        # 第二層的動態方程式(activation function)輸出(a)\n        # o(a) = 動態方程式(activation function)用Sigmoid function算法\n        o = self.activationSigmoid(self.z3)\n\n        # 回傳出前饋結果\n        return o \n backwardPropagate(反向傳播) \n def backwardPropagate(self, X, y, o):\n        # 計算輸出誤差\n        self.o_error = y - o\n\n        # 將Sigmoid function算法用在輸出誤差(錯誤、error)\n        self.o_delta = self.o_error*self.activationSigmoidPrime(o)\n\n        # 隱藏層的輸出誤差*權重\n        self.z2_error = self.o_delta.dot(self.W2.T)\n\n        # 將Sigmoid function算法用在隱藏層輸出誤差(錯誤、error)\n        self.z2_delta = self.z2_error*self.activationSigmoidPrime(self.z2)\n\n        # 糾正第一層權重數值，input--->hidden\n        self.W1 += X.T.dot(self.z2_delta)\n        # 糾正第二層權重數值，hidden--->output\n        self.W2 += self.z2.T.dot(self.o_delta) \n trainNetwork(訓練流程) \n def trainNetwork(self, X, y):\n        # 前饋循環 \n        o = self.feedForward(X)\n        # 反向傳播值\n        self.backwardPropagate(X, y, o) \n activationSigmoid \n def activationSigmoid(self, s):\n        # activation function\n        # 使用Sigmoid function算法(S-curve)\n        return 1/(1+np.exp(-s)) \n \n activationSigmoidPrime \n def activationSigmoidPrime(self, s):\n        # First derivative of activationSigmoid\n        # calculus time!\n        return s * (1 - s) \n \n saveSumSquaredLossList(儲存損失函數值) \n  def saveSumSquaredLossList(self,i,error):\n        lossFile.write(str(i)+","+str(error.tolist())+\'\\n\') \n saveWeights(儲存權重值) \n  def saveWeights(self):\n        np.savetxt("weightsLayer1.txt", self.W1, fmt="%s")\n        np.savetxt("weightsLayer2.txt", self.W2, fmt="%s") \n predictOutput(結果輸出) \n def predictOutput(self):\n        print ("Predicted XOR output data based on trained weights: ")\n        print ("Expected (X1-X3): \\n" + str(xPredicted))\n        print ("Output (Y1): \\n" + str(self.feedForward(xPredicted))) \n Epochs(疊代次數，feedForward+backprogation運算完算一次疊代) \n # 訓練疊代次數\ntrainingEpochs = 1000 \n TensorFlowKeras.py \n origin\xa0codes: \n import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Activation, Dense\nimport numpy as np\n# X = input of our 3 input XOR gate\n# set up the inputs of the neural network (right from the table)\nX = np.array(([0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,0], [1,1,1]), dtype=float)\n# y = our output of our neural network\ny = np.array(([1], [0], [0], [0], [0], [0], [0], [1]), dtype=float)\nmodel = tf.keras.Sequential()\nmodel.add(Dense(4, input_dim=3, activation=\'relu\', use_bias=True))\n#model.add(Dense(4, activation=\'relu\', use_bias=True))\nmodel.add(Dense(1, activation=\'sigmoid\', use_bias=True))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\', metrics=[\'binary_accuracy\'])\nprint (model.get_weights())\nhistory = model.fit(X, y, epochs=2000, validation_data = (X, y))\nmodel.summary()\n# printing out to file\nloss_history = history.history["loss"]\nnumpy_loss_history = np.array(loss_history)\nnp.savetxt("loss_history.txt", numpy_loss_history, delimiter="\\n")\nbinary_accuracy_history = history.history["binary_accuracy"]\nnumpy_binary_accuracy = np.array(binary_accuracy_history)\nnp.savetxt("binary_accuracy.txt", numpy_binary_accuracy, delimiter="\\n")\nprint(np.mean(history.history["binary_accuracy"]))\nresult = model.predict(X ).round()\nprint (result) \n 定義input(X)和output(Y) \n X = np.array(([0,0,0],[0,0,1],[0,1,0], [0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]), dtype=float)\n# X是三輸入XOR邏輯閘\ny = np.array(([1], [0], [0], [0], [0], [0], [0], [1]), dtype=float)\n# Y是輸出神經網路 \n setting \n model = tf.keras.Sequential() #sequential定義modle為層狀結構\nmodel.add(Dense(4, input_dim=3, activation=\'relu\', use_bias=True))\n\'\'\'\nadd是從最上層開始加入，Dense是密集連線的神經網路，\n4:輸出空間(神經元，輸出到4個神經元)，input_dim:輸入神經元個數，activation:定義啟動函數使用的類型，use_bias:使用偏差，True開啟。從inputlayer輸出到hiddenlayer的設定\n\'\'\'\n\n#model.add(Dense(4, activation=\'relu\', use_bias=True))\nmodel.add(Dense(1, activation=\'sigmoid\', use_bias=True))\n# 從hiddenlayer輸出到outputlayer的設定\n\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\', metrics=[\'binary_accuracy\'])\n\'\'\'\n配置訓練模組，loss funsion:用maen squared error(差平方誤差)，optimizer:優化器，\n用adam function，metrics：計算準確率，用binary_accuracy\n\'\'\'\n\nprint (model.get_weights())#印出回傳的正確權重\nhistory = model.fit(X, y, epochs=2000, validation_data = (X, y))\n\'\'\'\n訓練模型給予固定epochs，迭代收集到的資料，validation_data：評估準確率(不包含在訓練裡面)\n\'\'\'\n \n ReLU max_value：輸出後最大值上限 negative_slope：負斜率係數 threshold：可通過的數值界線 \n [ tf.keras.layers.ReLU | TensorFlow Core v2.4.0 ] \n mean squared error(MSE) \n \n ( 圖片來源 ) \n \n [ Proof (part 1) minimizing squared error to regression line (video) | Khan Academy ] \n [ 15 Types of Regression in Data Science ] \n [ tf.keras.Sequential | TensorFlow Core v2.4.0 ] \n [ Machine learning: an introduction to mean squared error and regression lines ] \n [ tf.keras.layers.Dense | TensorFlow Core v2.4.0 ] \n [ Module: tf.keras.losses | TensorFlow Core v2.4.0 ] \n [ Module: tf.keras.optimizers | TensorFlow Core v2.4.0 ] \n [ Module: tf.keras.metrics | TensorFlow Core v2.4.0 ] \n [ hub.KerasLayer | TensorFlow Hub ] \n [ Module: tf.summary | TensorFlow Core v2.4.0 ] \n [ Keras documentation: Layer activation functions ] \n history \n model.summary()\n# 摘要資料使用在分析和可視化，為了確認訓練架構在符合預期方向\n\nloss_history = history.history["loss"]\n#回傳紀錄事件(loss)到history物件，取得fit方法的模組回傳值\nnumpy_loss_history = np.array(loss_history)\n#將loss_history數值存成array\nnp.savetxt("loss_history.txt", numpy_loss_history, delimiter="\\n")\n#將numpy_loss_history存成loss_history.txt，並將每筆資料用換行符號隔開\n\nbinary_accuracy_history = history.history["binary_accuracy"]\n#回傳紀錄事件(binary_accuracy)到history物件\nnumpy_binary_accuracy = np.array(binary_accuracy_history)\n#將binary_accuracy_history數值存成array\nnp.savetxt("binary_accuracy.txt", numpy_binary_accuracy, delimiter="\\n")\n#將numpy_binary_accuracy存成binary_accuracy.txt，並將每筆資料用換行符號隔開\n \n result \n print(np.mean(history.history["binary_accuracy"]))\n#印出平均binary_accuracy記錄到的數值\nresult = model.predict(X ).round()\n#替輸入樣本產生輸出預測\nprint (result)\n#印出結果 \n 整理好的PDF檔 \n', 'tags': '', 'url': 'neural_network_in_python.pdf.html'}, {'title': '參考範例', 'text': '參考範例\xa0 https://github.com/mdecourse/4072pj1/blob/master/40723150/example/pong2.py \n GPU運算 \n with cp.cuda.Device(0):#用GPU0做計算\n    if resume:\n        model = pickle.load(open(\'save.p\', \'rb\'))\n        \'\'\'以\xa0Binary 的方式讀取\xa0save.p\'\'\'\n        print(\'resuming\')\n    else:\n        model = {}\n        model[\'W1\'] = np.random.randn(D,H) / np.sqrt(D) \n\t\'\'\'W1檢測遊戲場景\'\'\'\n        model[\'W2\'] = np.random.randn(H,A) / np.sqrt(H)\n\t\'\'\'W2決定擊錘向上或向下移動\'\'\'\n    grad_buffer = { k : np.zeros_like(v) for k, v in model.items() }\n    # update buffers that add up gradients over a batch\n    rmsprop_cache = { k : np.zeros_like(v) for k, v in model.items() }\n    # rmsprop memory\n \n \n \xa0python open()[來源： w3schools ] \n sigmoid\xa0 \n def sigmoid(x): \n    return 1.0 / (1.0 + np.exp(-x)) \n $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ \n softmax \n def softmax(x):\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    return probs \n $$\\sigma(\\overrightarrow{z})_i = \\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}}$$ \n [來源： softmax 資料] \n prepro \n def prepro(I):\n    """ prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """\n    I = I[35:195] # 裁掉頂部的計分和底部空白，剩下160x160\n    I = I[::2,::2,0] # 從160x160縮減到80x80，取出第一個顏色channel\n    I[I == 144] = 0 # 將背景顏色變成黑色\n    I[I == 109] = 0 # 將背景顏色變成黑色\n    I[I != 0] = 1 # 球和擊錘變成白色\n    return I.astype(np.float).ravel() \n discount rewards \n def discount_rewards(r):\n    """ take 1D float array of rewards and compute discounted reward """\n    discounted_r = np.zeros_like(r) #和r矩陣相同維度大小的0矩陣\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        if r[t] != 0: running_add = 0 \n        # 當球到邊界的時候重設總和\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n\t\t    \'\'\'加入第t項折扣\'\'\'\n    return discounted_r \n policy forward \n def policy_forward(x):\n    if(len(x.shape)==1):\n        x = x[np.newaxis,...]\n\n    h = x.dot(model[\'W1\'])\n    h[h<0] = 0 # ReLU nonlinearity\n    logp = h.dot(model[\'W2\'])\n    #p = sigmoid(logp)\n    p = softmax(logp)\n\n    return p, h \n    # return probability of taking action 2, and hidden state\n    # h 是球在環境上的狀態，p 是移動的決策\n \n policy backward \n def policy_backward(eph, epdlogp):\n    """ backward pass. (eph is array of intermediate hidden states) """\n    dW2 = eph.T.dot(epdlogp)  \n    dh = epdlogp.dot(model[\'W2\'].T)\n    dh[eph <= 0] = 0 # backpro prelu\n\n    t = time.time()\n    # problem: https://github.com/chainer/chainer/issues/8582\n    if(be == cp): #將參數複製到GPU或CPU\n        dh_gpu = cuda.to_gpu(dh, device=0)\n        epx_gpu = cuda.to_gpu(epx.T, device=0)\n        dW1 = cuda.to_cpu( epx_gpu.dot(dh_gpu) )\n        # 將GPU的矩陣複製到CPU\n    else:\n        dW1 = epx.T.dot(dh) \n    \n\n    print((time.time()-t0)*1000, \' ms, @final bprop\')\n\n    return {\'W1\':dW1, \'W2\':dW2} \n 訓練過程 \n while True:\n    t0  = time.time()\n    #render = True\n    if render: \n        t  = time.time()\n        env.render()# 顯示訓練中狀況(影像)\n        print((time.time()-t)*1000, \' ms, @rendering\')\n        # 計算每幀時間差\n\n    t  = time.time()\n    # preprocess the observation, set input to network to be difference image\n    cur_x = prepro(observation)\n    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n    prev_x = cur_x\n  \n    # forward the policy network and sample an action from the returned probability\n    t  = time.time()\n    aprob, h = policy_forward(x)\n\n    # roll the dice, in the softmax loss\n    u = np.random.uniform() # 隨機取出一個數(擲骰子)\n    aprob_cum = np.cumsum(aprob) # 三個動作的機率\n    a = np.where(u <= aprob_cum)[0][0]\n    \'\'\'\n    骰子的點數落在哪個行為的區間，舉例:\n    u = 0.5826051448555892\n    aprob_cum  = [0.99999548 0.99999872 1. ]\n    u <  aprob_cum[0]，則執行aprob_cum[0]的行為\n    \'\'\'\n    action = a+2\n    #print(u, a, aprob_cum)\n  \n    # record various intermediates (needed later for backprop)\n    t = time.time()\n    xs.append(x) # observation\n    hs.append(h) # hidden state\n\n    #softmax loss gradient\n    dlogsoftmax = aprob.copy()\n    dlogsoftmax[0,a] -= 1 #-discounted reward \n    dlogps.append(dlogsoftmax)\n\n\n    # step the environment and get new measurements\n    t  = time.time()\n    observation, reward, done, info = env.step(action)\n    reward_sum += reward \n\n    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n\n\n    if done: # an episode finished\n        episode_number += 1\n\n        t  = time.time()\n\n        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n        epx = np.vstack(xs)\n        eph = np.vstack(hs)\n        epdlogp = np.vstack(dlogps)\n        epr = np.vstack(drs)\n        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n\n        print(epdlogp.shape)\n\n        # compute the discounted reward backwards through time\n        discounted_epr = discount_rewards(epr)\n        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n        discounted_epr -= np.mean(discounted_epr)\n        discounted_epr /= np.std(discounted_epr)\n\n        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n        grad = policy_backward(eph, epdlogp)\n        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n\n        # perform rmsprop parameter update every batch_size episodes\n        if episode_number % update_freq == 0: #update_freq used to be batch_size\n            for k,v in model.items():\n                g = grad_buffer[k] # gradient\n                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n                model[k] -= learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n\n        # boring book-keeping\n        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n        print(\'resetting env. episode reward total was %f. running mean: %f\' % (reward_sum, running_reward))\n        if episode_number % 100 == 0: pickle.dump(model, open(\'save.p\', \'wb\'))\n        reward_sum = 0\n        observation = env.reset() # reset env\n        prev_x = None\n        \n        print((time.time()-t)*1000, \' ms, @backprop\')\n\n\n    outstring =""\n\n    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n        if reward == -1:\n            outstring = \'\'\n        else:\n            outstring = \'!!!!!!!\'\n        \n        print (\'ep \'+ str(episode_number) + \': game finished, reward:\' +str(reward)+ outstring ) \n 訓練結果 \n 點是每局的獎勵總和，曲線是累積獎勵平均值。橫軸：訓練局數；縱軸獎勵分數。紅點是pong1.2，綠點是pong2。(圖1.)pong2的activation function為softmax， pong1.2則是sigmoid。 \n \n 圖1. pong2 v.s. pong1.2 \n [ pong2_vs_pong1.2.pdf ] \n \n \n \n', 'tags': '', 'url': '參考範例.html'}, {'title': 'Markov', 'text': "Markov Chain \n 當前決策只會影響下個狀態，當前狀態轉移(action)到其他狀態的機率有所差異。 \n Markov Reward Process \n action 到指定狀態會獲得獎勵。 \n $$R(s_t=s) = \\mathbb{E}[r_t|s_t = s]$$ \n $$\\gamma \\in [0, 1]$$ \n \n Horizon： 在無限的狀態以有限的狀態表示 \n Return： 越早做出正確決策獎勵越高 $$G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+...+\\gamma^{T-t-1} R_{T}$$ \n State value function： 決策價值 $$V_t(S) = \\mathbb{E}[G_t|s_t = s]$$ \n \n Discount Factor (γ) \n \n 越早做出有獎勵的決策，獎勵越高 \n 做出有價值的決策 $\\gamma = 1$，不分決策順序先後 \n 無用的決策C= 0，不會得到獎勵 \n \n Bellman equation：描述互動關係狀態 \n $$V(s) = R(s)+\\gamma\\sum_{s'\\in S}P(s'|s)V(s')$$ \n $$R(s):立即獎勵$$ \n $$\\gamma\\sum_{s'\\in S}P(s'|s)V(s')：未來獎勵衰減總和$$ \n Anaytic solution ：分析性解法 MRP的分析性解法： \n $$V = (1-\\gamma P)^{-1}R$$ \n 只適合小的MRP(個數比較少的)， 矩陣複雜度為$O(N^3)$，N為狀態個數 \n 計算大型MRP會使用疊代法： \n \n 動態規劃(Dynamic programming) \n Mote-Carlo evaluation 以評估採樣的方式 $$g = \\sum_{i=t}^{H-1}\\gamma^{1-t}r_i$$ $$G_t \\leftarrow G_t+g,\xa0 i \\leftarrow i+1$$ $$V_t(s) \\leftarrow \\frac{G_t}{N}$$ \n Temporal-Difference learning \n \n", 'tags': '', 'url': 'Markov.html'}, {'title': 'Markov Decision Process', 'text': "在MRP中加入決策(decision)和動作(action) \n \n S：state 狀態 \n A：action 動作 \n P：狀態轉換 $$P(s_{s+1}=s'|s_t=s,a_t=a)$$ \n R：獎勵，取決於當前狀態和動作會得到相對應的講勵 $$ R(s_t=s, a_t=a) =\xa0\\mathbb{E}[r_t|s_t, a_t=a]\xa0$$ \n D：折扣因子(discount factor) $$\\gamma \\in [0,1]$$ \n \n MDP is a tuple：(S, A, P, R, $\\gamma$) \n policy (決策)：可以是一個決策行為機率或確定行為 \n $$\\pi (a|s) = P(a_t=a|s_t=s)$$ \n MRP 和 MDP 轉換 \n \n \n \n MRP \n ↔ \n MDP \n \n \n $P^{\\pi}(s's) = \\sum_{a\\in A}\\pi (a|s)P(s'|s, a)$ \n \n \n $P^{\\pi}(s) = \\sum_{a\\in A}\\pi (a|s)P(s, a)$ \n \n \n \n \n 圖1.MRP \n \n \n \n \n 圖2.MDP \n \n \n \n \n state value function(狀態值方程式) $v^{\\pi}(s)$ \n $$v^{\\pi}(s) = \\mathbb{E}[G_t|s_t=s]$$ \n $$= \\mathbb{E}[R_{t+1}+\\gamma v^{\\pi}(s_{t+1})|s_t=s]$$ \n $$= \\sum_{a\\in A}\\pi (a|s)q^{\\pi}(s, a)$$ \n \n 圖3.Backup Diagram for $V^{\\pi}$ \n $$v^{\\pi}(s) = \\sum_{a\\in A}\\pi (a|s)(R(s, a)+\\gamma \\sum_{s'\\in s}P(s'|s, a)v^{\\pi}(s'))$$ \n action value function(行為方程式)$q^{\\pi}(s, a)$ \n $$q^{\\pi}(s, a) = \\mathbb{E}[G_t|s_t=s, A_t=a]$$ \n $$= \\mathbb{E}[R_{t+1}+\\gamma q^{\\pi}(s_{t+1}, A_{t+1})|s_t=s, A_t=a]$$ \n $$= R^a_s+\\gamma \\sum_{s'\\in s}P(s'|s)v^{\\pi}(s')$$ \n $$= R(s, a)+\\gamma \\sum_{s'\\in s}P(s'|s)\xa0\\sum_{a'\\in A}\\pi (a'|s')q^{\\pi}(s', a')$$ \n \n 圖4.Backup Diagram For $Q^{\\pi}$ \n $$q^\\pi(s, a)=R(s, a)+\\gamma\\sum_{s'\\in S}P(s'|s, a)\\sum_{a'\\in A}\\pi(a'|s')q^{\\pi}(s', a')$$ \n", 'tags': '', 'url': 'Markov Decision Process.html'}, {'title': 'Reinforcement Learning Basics', 'text': '\n', 'tags': '', 'url': 'Reinforcement Learning Basics.html'}, {'title': '優化器', 'text': '為了讓AI學習的錯誤率降低，因此利用優化器來降低loss function的值，在error surface上找到最小值，即是找到錯誤率最低的地方。以下將介紹幾種優化的方法： \n Gradient Descent \n 利用梯度的方式尋找最小值的位置，其特色可找到凸面error surface的絕對最小值，在非凸面error surface上找到相對最小值。其缺點是在非凸面error surface要避免被困在次優的局部最小值。 \n Batch gradient descent \n 用批次的方式計算訓練資料，整個資料集計算梯度只更新一次，因此計算和更新時會占用大量記憶體。整體效率較差、速度較緩慢。由 Gradient Descent 延伸出來的算法。其收斂行為與 \xa0Gradient Descent\xa0相同。 \n Stochastic gradient descent \n 每次執行時會更新並消除誤差，有頻繁更新和變化大的特性，較不容易困在特定區域。 由 Gradient Descent 延伸出來的算法。其收斂行為與 \xa0Gradient Descent\xa0相同。 \n Mini-batch gradient descent \n 結合\xa0Batch gradient descent 和\xa0Stochastic gradient descent 的特點：批量計算和頻繁更新，所衍伸的算法。利用小批量的方式頻繁更新，並使收斂更穩定。其缺點：學習率挑選不易、預定義 threshold\xa0無法適應數據集的特徵、對很少發生的特徵無法執行較大的更新、 非凸面error surface要避免被困在次優的局部最小值等。 \n Gradient descent optimization algorithms \n 為了改善前面幾種算法而發展出來的優化算法。以下將列出數種優化算法。 \n Momentum \n 在梯度下降法加上動量的概念，會加速收斂到最小值並減少震盪。 \n Nesterov accelerated gradient \n NAG，有感知能力的 Momentum：在坡度變陡時減速，避免衝過最小值所造成的震盪(為了修正到最小值，來回修正而產生的震盪) \n Adagrad \n 其學習率能適應參數： 頻繁出現的特徵用較低的學習率，不經常出現的特徵則用較高的學習率，且無須手動調整學習率。其缺點是，學習率會急遽下降，最後會無限小，這算法就不再獲得知識。 \n Adadelta \n 為 Adagrad 的延伸，下降激進程度，學習率從更新規則中淘汰，不需設定預設學習率。 \n RMSprop \n 為了解決 Adagrad 學習率急劇下降的問題，學習率除以梯度平方的RMS，解決學習率無限小的情形。 \n Adam \n 類似 Momentum，更加穩定快速的收斂。 \n AdaMax \n 與 Adam 相似，依靠 (u_t) \xa0 最大運算 \n Nadam \n 結合 Adam 和 NAG ，應用先前參數執行兩次更新，一次更新參數一次更新梯度。 \n AMSGrad \n 改善 Adam 算法所導致收斂較差的情況(用指數平均會減少其影響)，換用梯度平方最大值來做計算，並移除去偏差的步驟。是否有比\xa0 Adam 算法好仍有待觀察。 \n Gradient noise \n 有助於訓練特別深且復雜的網絡，noise 可改善不良初始化的網路。 \n 神經網路優化器算法整理 \n', 'tags': '', 'url': '優化器.html'}, {'title': 'Gradient Descent Optimizer', 'text': '( 資料來源 ) \n \n 圖1.\xa0Error_surface ( 圖片來源 ) \n 藉由梯度下降將目標函數值最小化，目標函數以loss function $L(\\theta)$為例，$\\theta$為weight $(w)$ 和 bias $(b)$ 的向量函數，為了找到error surface(圖1)上的最小值，因此加上$\\Delta\\theta$ 將$\\theta$的方向修正並引導到正確方向，避免每次修正的過多導致錯過最小值，利用係數$\\eta$(學習率)縮放 $\\Delta\\theta$的修正量(圖2)，修正後方程式為： \n $$\\theta=\\theta+\\eta\\cdot\\Delta\\theta$$ \n \n 圖2. Theta_vector ( 圖片來源 ) \n 將 $\\theta$以 泰勒展開式 表示，假設並$\\Delta\\theta$為u: \n $$L_{(\\theta+\\eta u)}=L_{(\\theta)}+\\eta u^{T}\\cdot\\bigtriangledown_{\\theta} L_{(\\theta)}+\\frac{\\eta^2}{2!}u^T\\cdot\\bigtriangledown^2 L_{(\\theta)}u+\\frac{\\eta^3}{3!}...+\\frac{\\eta^4}{4!}...+\\frac{\\eta^n}{n!}...$$ \n 以泰勒展開式的型式表示的好處是： $\\theta$ 些微的更動產生新值。$\\eta$ 值通常小於一，當 $\\eta^2 << 1$，因此可以忽略高階項\xa0 \n $$L_{(\\theta+\\eta u)}=L_{(\\theta)}+\\eta u^{T}\\cdot\\bigtriangledown_{\\theta} L_{(\\theta)} [\\eta\\ is\\ typically\\ small, so\\ \\eta^2, \\eta^3,\\cdots \\rightarrow 0]$$ \n 新的$ L(\\theta\xa0+ \\eta u)$ 輸出的值會小於$ L(\\theta) L(\\theta+\\eta u) − L(\\theta) < 0$，同理可證 $u^T\\cdot\\bigtriangledown\\theta L(\\theta)$ < 0，符合u這條件：當新的值小於舊的值，u就是一個好的值。假設u和$\\bigtriangledown\\theta L(\\theta)$ 的夾角為$\\href{https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length}{\\underline{\\beta}}$\xa0 \n $$\\cos(\\beta)=\\frac{u^{T}\\cdot\\bigtriangledown_{\\theta} L_{(\\theta)}}{\\vert u^{T}\\vert\\vert \\bigtriangledown_{\\theta} L_{(\\theta)}\\vert}$$ \n 因為 $\\cos(\\theta)$ 的值介於 1 和-1 之間 \n $$-1<\\cos(\\beta)=\\frac{u^{T}\\cdot\\bigtriangledown_{\\theta} L_{(\\theta)}}{\\vert u^{T}\\vert\\vert \\bigtriangledown_{\\theta} L_{(\\theta)}\\vert}\\leq 1$$ \n $$k=\\vert u^{T}\\vert\\vert \\bigtriangledown_{\\theta} L_{(\\theta)}\\vert $$ $$-k \\leq k\\cos(\\beta)=u^{T}\\cdot\\bigtriangledown_{\\theta} L_{(\\theta)}\\leq k$$ \n 所以盡可能的讓新值小於舊值 $(L(\\theta+\\eta u) − L(\\theta) < 0)$，loss 值就會減少得越多。因此 $u T \\cdot \\bigtriangledown\\theta L(\\theta)$\xa0應該為負，在 這情況下 $\\cos(\\beta)$ 等於 −1，$\\beta$ 的角度為 180 ◦ ，這就是$\\theta$移動的方向與梯度方向相反的原因。 梯度下降法告訴我們：當 $\\theta$ 在特定值，並想減少新的 $\\theta$ 值，使 loss 值逐漸減少就應該與梯度相反的方向找 (若梯度為正值，找最小值就需往負的方向找)  \n $w_{t=1}=w_t-\\eta\\bigtriangledown w_t$ $b_{t=1}=b_t-\\eta\\bigtriangledown b_t$ $where\\ at\\ w=w_t,b=b_t$ $  \\begin{cases}  \\bigtriangledown w_t=\\frac{\\partial L_{_{(\\theta)}}}{\\partial w}\\\\  \\bigtriangledown b_t=\\frac{\\partial L_{_{(\\theta)}}}{\\partial b}  \\end{cases} $ \n', 'tags': '', 'url': 'Gradient Descent Optimizer.html'}, {'title': 'Stochastic gradient descent', 'text': '( 資料來源 ) \n Batch gradient descrnt \n Vanilla gradient descent 又稱 Batch gradient descent(批次梯度下降法)，計算目標函數的梯度，參數 θ 對於整個 訓練資料： \n $$\\theta=\\theta-\\eta\\cdot\\bigtriangledown_{\\theta}L_{(\\theta)}$$ \n 目標函數以為例 loss function L(θ)，參數 $\\theta$為 weight (w) 和 bias (b) 的函數，$\\eta$為學習率。由於計算整個資料集計算梯度只更新一次，Bath gradient descent 可能非常慢並且對於資料集無法符合及記憶體來說棘手 (一次需要儲存整個資料集的資料，當更新和計算時會占用大量記憶體)。 \n for i in range(nb_epochs) :\nparams_grad = evaluate_gradient (loss_function, data, params)\nparams = params − learning_rate ∗ params_grad\n \n 程式1.\xa0Batch gradient descrnt\xa0 ( 程式來源 ) \n 預定義每次 epoch，先計算 loss function 梯度向量對於整個資料集參數向量。如果梯度值來自於先前計算出的梯度值，就會檢查梯度，並以梯度相反的方向更新參數$\\theta$，學習率$\\eta$決定多大的更新量。Batch gradient descent 對於凸面誤差可以保證收斂到廣域最小值，對於非面凸誤差可以收斂到局部最小值。 \n Stochastic gradient descent \n Stochastic gradient descent(SGD) 隨機梯度下降法，這裡的目標函數為 $J(\\theta, x^i, y^i)$(變數 $\\theta$ 為 w(weight) 和 b(bias) 的函數，也可以寫成 $ J(w,b,\\theta, x^i, y^i)$ ) 。 \n $$\\theta=\\theta-\\eta\\cdot\\bigtriangledown_{\\theta}J_{(\\theta, x^i, y^i)}$$ \n 批量梯度下降他會在每個參數更新前重新計算相似梯度。SGD 每次次執行會更新來消除多餘 (誤差)，因此通常速度 很快。SGD 頻繁更新並變化很大，因為目標方程式波動很大 (圖1)。 \n \n 圖1.SGD fluctuation\xa0 ( 圖片來源 ) \n SGD 的方程式一方面會跳到新的值和潛在局部最小值，另一方面 SGD 會持續超調 (誤差超過預期) 最後收斂到廣域最小值。無論如何他被顯示當學習率下降緩慢，SGD 顯示與 Batch gradient descent 同樣收斂行為，幾乎可以肯定地，對於凸面或非凸面優化，會收斂到絕對或是局部最小值。這程式碼片段 (程式.2) 在訓練樣本上加入一個迴圈來對每個樣本評估梯度。每個 epoch(訓練循環) 會打亂訓練數據。 \n for i in range(nb_epochs) :\nnp.random.shuffle(data)\nfor example in data :\nparams_grad = evaluate_gradient (loss_function, example, params)\nparams = params − learning_rate ∗ params_grad \n 程式2.\xa0Stochastic\xa0gradient descrnt\xa0 ( 程式來源 ) \n Mini-batch gradient descent \n Mini-batch gradient descent(小批量梯度下降) 各取前兩者的優點，將資料集分割成小區塊，每個小區塊大小稱作 batch size，每次跑完 batch size 算迭代 (iteration)一次，算完一次資料集即完成一次 epoch。舉例: 資料集大小為 1000，若 batch size 為 50，iteration 為 datasets 的 batch_size = 1000÷50 = 20，當 iteration 跑完 20 次算完成一次 epoch。 \n 這方式可以減少參數更新的方差，並且可以穩定收斂；可利用深度學習庫所共有的高度優化的矩陣優化，從而由一個小批量計算出梯度非常有效。通常 batch sizes 的範圍介於 50 ~256，會因為應用而有所差異。訓練神經網絡時，通常選擇 Mini-batch gradient descent 算法，而當使用這算法時，通常也用 SGD 稱呼。 \n $$\\theta=\\theta-\\eta\\cdot\\bigtriangledown_{\\theta}J_{(\\theta, x^{(i:i+n)}, y^{(i:i+n)})}$$ \n 下面程式碼 (程式.3) 為迭代範例，batch size 大小為 50： \n for i in range(nb_epochs ) :\nnp.random.shuffle(data)\nfor batch in get_batches (data, batch_size = 50):\nparams_grad = evaluate_gradient (loss_function, batch, params)\nparams = params − learning_rate ∗ params_grad\n \n 程式3.  Mini-batch  gradient descrnt\xa0 ( 程式來源 ) \n Challenges \n Mini-batch gradient descent 無論如何還是無法確保收斂的很好，存在一些需要解決的挑戰： \n \n 選擇適當的學習率是有難度的。如果學習率太小會導致收斂困難或緩慢，學習率太大則會阻礙收斂導致 loss function 來回波動或發生偏離。 \n \xa0學習率清單嘗試在訓練的時候調整學習率，即根據預定義清單或當目標下降於閾值 (threshold) 時降低學習率。 但清單和閾值須預先定義，因此無法適應數據集的特徵。 \n \xa0另外相同學習率適用全部參數更新。如果資料稀疏而且外型有很特別的頻率，我們可能不希望將所有特徵更新 到相同的程度，而是對很少發生的特徵執行較大的更新。 \n \xa0最小化神經網路常見的高度非凸面誤差方程式  (error function)  的另一關鍵挑戰則是要避免被困在大量次優的局 部最小值區域中。認為困難實際上不是由局部最小值引起的，而是由鞍點引起的，即一維向上傾斜而另一維向下 傾斜的點。這些鞍點通常被相同誤差的平穩段包圍，這使得 SGD 很難逃脫，因為在所有維度上梯度都接近於零。 \n \n', 'tags': '', 'url': 'Stochastic gradient descent.html'}, {'title': 'Gradient descent optimization algorithms', 'text': '( 資料來源 ) \n Momentum \n SGD 難以在陡峭的往正確的方向，那就是說在一個維度上，曲面的彎曲比另一個維度要陡得多，這在局部最優情況下很常見。下圖(圖.1)的同心圓代表中心下凹的曲面。在這些情況下，SGD 會在陡峭的地方振盪，而僅沿著底部朝著局部最優方向猶豫前進，如 (圖.1.a) 所 示。 Momentun(動量) 是一個幫助加速 SGD 在正確方向和抑制震盪的方法，在 (圖.1.b)。 \n \n \n \n \n \n \n \n \xa0圖.1. a  SGD without momentum \n 圖.1. b  SGD with momentum\xa0 \n \n \n \n ( 圖片來源 ) \n \n \n \n 這麼做會增加一個係數 $\\gamma$ (gamma) 來更新上次的向量到正確向量 (修正偏差)，$\\gamma$ 通常設為 0.9 左右。 \n $$v_t = \\gamma v_{t-1}+\\eta\\cdot\\bigtriangledown_{\\theta}J_{(\\theta)}$$ \n $$\\theta = \\theta-v_t$$ \n 實際上，使用動量的時候，就像將球推下山坡。球在下坡時滾動時會累積動量，在途中速度會越來越快（如果存在空氣阻力，直到達到極限速度，也就是\xa0$\\gamma < 1$) 參數更新也發生了同樣的事情：動量 (momentum) 對於梯度指向相同方向的維度增加，而對於梯度改變方向的維減少動量。結果，我們獲得了更快的收斂並減少了振盪。 \n Nesterov accelerated gradient \n Nesterov accelerated gradient（NAG）是一種使動量具有一個去向的概念，以便在山坡再次變高之前知道它會減速。我們知道使用動量 $\\gamma v_{t-1}$\xa0來移動參數。計算 $\\theta - \\gamma v_{t-1}$\xa0這樣就給了參數的下一個位置的近似值（完整更新缺少的梯度），這是參數將要存在的大致概念。現在，通過計算與當前參數無關的梯度來有效地看到目前的參數 $theta$ 將會移動到的位置： \n $$v_t = \\gamma v_{t-1}+\\eta\\cdot\\bigtriangledown_{\\theta}J_{(\\theta-\\gamma v_{t-1})}$$ \n $$\\theta = \\theta - v_t$$ \n \n 圖.2 NAG\xa0 ( 圖片來源 ) \n 同樣，我們設置動量 $\\gamma$ 約為 0.9。動量首先計算當前梯度（(圖.2) 中的藍色小向量），然後在更新的累積梯度（藍 色向量）的方向上發生較大的跳躍，而 NAG 首先在先前的累積梯度的方向上進行較大的跳躍（棕色向量），測量梯度，然後進行校正（紅色向量），從而完成 NAG 更新（綠色向量）。這種預期的更新可防止我們過快地進行，並導致響應速度增加，從而顯著提高了 RNN 在許多任務上的性能。有關 NAG 背後另一解釋，請 參見此處 ，而 Ilya Sutskever 在其博士論文中給出了更詳細的概述。 \n Adagrad \n Adagrad 是一個梯度優化的算法，它可以做到：學習率適應參數，對於頻繁出現的特徵相關參數執行較小的更新(較低的學習率)，以及對不經常出現的特徵相關參數進行較大更新（即學習率較高）。Adagrad 可以提高 SGD 的強度，用於訓練大型神經網絡。 \n 先前，在同一次 $theta$ 參數(更新後就算另一次)，每個 $theta$ 都使用相同的 $\\eta$ (學習率)。Adagrad 則是對每個 $theta$ 參數使用 不同的 $\\eta$，t 代表 time step。先將 Adagrad 的更新參數向量化。用 $g_t$\xa0表示 time step 的梯度，$g_{t,i}$\xa0表示目標函數 (參數 $theta$ 在 time step t) 對參數做偏微分計算。 \n $$g_{t,i}=\\bigtriangledown_{\\theta}J_{(\\theta_{t,i})}$$ \n 當 SGD 更新每個參數 $\\theta_i$，在每個 time step t，因此變成： \n $$\\theta_{t+1,i}=\\theta_{t,i}-\\eta\\cdot g_{t,i}$$ \n 更新規則，Adagrad 根據先前  $\\theta_i$ \xa0計算的梯度，對每個參數  $\\theta_i$ \xa0修改整個學習率 $\\eta$ 在每個 time stept： \n $$\\theta_{t+1,i}=\\theta_{t,i}-\\frac{\\eta}{\\sqrt{G_{t,ii}+\\epsilon}}$$ \n $G_t \\in \\mathbb{R}^{d\\times d}$ 這是一個對角矩陣每個對角元素 i，i 是關於 $theta$ 梯度平方和取決於 time stept，$\\epsilon$ 是避免分母為0($\\epsilon$ 通常 為 $10^{-8}$\xa0)，如果沒有平方根運算，該算法的性能將大大降低。 $G_t$\xa0包含了過去梯度平方根，由於全部 $theta$ 參數沿著對角線，通過向量的內積計算 $G_t$\xa0和 $g_t$： \n $$\\theta_{t+1}=\\theta_{t}-\\frac{\\eta}{\\sqrt{G_{t}+\\epsilon}}\\cdot g_t$$ \n Adagrad 主要好處之一是，無需手動調整學習率。大多數實現使用預設值 0.01 並將其保留為預設值。Adagrad 主 要弱點是會累積分母的平方梯度：由於每項都是正的，累積和會在訓練中不斷增長。反過來，學習率下降，並最終變 得無限小，這算法就不再獲得知識。 \n Adadelta \n Adadelta 是 Adagrad 的延伸，下降其激進的程度，單調的降低學習率。Adadelta 會限制過去累積的梯度，並將其 限制在某個特定大小 w，並代替 Adagrad 過去累積的梯度平方，以梯度總和是遞迴定義為所有過去衰減梯度平方平均值。流動平均 $E[g^2]_t$\xa0在 time step t 然後取決於 (像 Momentum 的 $\\gamma$) 先前 平均和最近梯度： \n $$E[g^2]_t=\\gamma E[g^2]_{t-1}+(1-\\gamma)g^2 _t$$ \n $\\gamma$ 值和 Momentum 的相似，約為 0.9，現在根據參數更新向量 $\\bigtriangleup\\theta_t$ 來重寫 SGD： \n $$\\bigtriangleup\\theta_t=-\\eta\\cdot g_{t,i}$$ \n $$\\theta_{t+1}=\\theta_t+\\bigtriangleup\\theta_t$$ \n Adagrad 的參數更新向量替換成：對角矩陣 G t  過去梯度平方的衰退平均  $E[g^2]_t$\xa0 \n $$\\bigtriangleup\\theta_t=-\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}\\cdot g_t$$ \n $$replace\\ G_t\\ with\\ E[g^2]_t\\Rightarrow\\bigtriangleup\\theta_t=-\\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}\\cdot g_t$$ \n 由於分母只是梯度的均方根 (RMS)，我們可以取代成縮寫： \n $$\\bigtriangleup\\theta_t=-\\frac{\\eta}{RMS[g]_t}\\cdot g_t$$ \n 這個更新單位和 SGD、Momentum 以及 Adagrad 的單位不符合，因此更新需有相同的參數。為了實現這一點，首先定義另一個指數衰減平均值，這次不是梯度平方更新而是參數平方更新： \n $$E[\\bigtriangleup\\theta^2]_t=\\gamma E[\\bigtriangleup\\theta^2]_{t-1}+(1-\\gamma)\\bigtriangleup\\theta^2 _t$$ \n RMS 參數更新: \n $$RMS[\\bigtriangleup\\theta]_t=\\sqrt{E[\\bigtriangleup\\theta^2]_t+\\epsilon}$$ \n $RMS[\\bigtriangleup\\theta]_t$ 是未知的，更新參數的 RMS 取近似值到上個 time step。用 $RMS[\\bigtriangleup\\theta]_t$\xa0取代學習率 $\\eta$，最後產生新 的規則： \n $$\\bigtriangleup\\theta_t=-\\frac{RMS[\\bigtriangleup\\theta]_{t-1}}{RMS[g]_t}g_t$$ $$\\theta_{t+1}=\\theta_t+\\bigtriangleup\\theta_t$$ \n 使用 Adadelta，甚至不需要設定預設學習率，因為它已從更新規則淘汰。 \n RMSprop \n RMSprop 是 Geoffrey Hinton 在他的課程中提出的未公開自適應學習率的方法。 \n RMSprop 和 Adadelta 都是為了解決 Adagrad 的學習率急劇下降的問題個別獨立開發出來的解決方式。RMSprop 實際上與 Adadelta 得出的第一個更新向量相同： \n $$E[g^2]_t=0.9E[g^2]_t+0.1g^2 _t$$ \n $$\\theta_{t+1}=\\theta_t-\\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}g_t$$ \n RMSprop 也將學習率除以梯度平方的指數衰減平均值。Hinton 建議 $\\gamma$ 設為 0.9，好的預設學習率 $\\eta$ 數值為 0.001。 \n Adam \n Adaptive Moment Estimation 自適應矩評估 (Adam) 是另一種計算每個評估學習率的方法。出了儲存過去梯度平 方的指數衰減平均值 $v_t$，就像 Adadelta 和 RMSprop 一樣，Adam 還保留過去梯度的指數衰減平均值 $m_t$，類似動量 (Momentum)。如果 Momentum 被視為順著斜坡下滑的球，而 Adam 則是像一個帶有摩擦的沉重的球，因此更適合待在 error face 平坦的最小值區域。計算過去梯度平方的衰減平均值 $m_t$\xa0和 $v_t$\xa0分別如下： \n $$m_t=\\beta_1 m_{t-1}+(1-\\beta_1)g_t$$ \n $$v_t=\\beta_2 v_{t-1}+(1-\\beta_2)g^2 _t$$ \n $m_t$  和  $v_t$  分別是第一階矩平均估計值和第二階矩無中心方差估計值，因此是方法的名稱。像  $m_t$  和  $v_t$ \xa0 被初始化為向量 o，Adam 的作者觀察到它們偏向零，特別是在初始 time step，尤其是在衰減率較小的時候 (也就是說 $\\beta_1$ 和 $\\beta_2$ \xa0 趨近於 1) 藉由計算校正偏差第一矩\xa0$\\hat{m}_t$ \xa0 和第二矩 $\\hat{v}_t$ \xa0 抵消偏差： \n $$\\hat{m}_t = \\dfrac{m_t}{1 - \\beta^t_1} $$ \n $$\\hat{v}_t = \\dfrac{v_t}{1 - \\beta^t_2} $$ \n 使用他們去更新參數，就像 Adadelta 和 RMSprop 中所看到的那樣，這將產生 Adam 更新規則： \n $$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$ \n $\\beta_1$  預設值建議為 0.9， $\\beta_2$ \xa0預設值建議為 0.999，ϵ 預設值建議為  $10^{-8}$ 。根據經驗證明 Adam 表現良好，並且與其他自適應學習算法相比具有優勢。 \n AdaMax \n 在 Adam 更新規則中的  $v_t$  係數是與梯度成反比地縮放過去梯度的範數 (通過  $v_{t-1}$ \xa0項) 和當前梯度 $|g_t|^2$ \xa0 ： \n $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) |g_t|^2$$ \n 我們轉換這個更新到 $\\ell_p$。注意$\\beta_2$參數化為$\\beta_2^p$： \n $$v_t = \\beta_2^p v_{t-1} + (1 - \\beta_2^p) |g_t|^p$$ \n 大規範 p 值使數值上變得不穩定，這就是為什麼 $\\ell_1$ 和 $\\ell_2$ 規範在實踐中是最常見的。然而，$\\ell_\\infty$ 通常也表現出穩定 的行為。作者 (Kingma and Ba, 2015) 提出了 AdaMax 並證明了和 $\\ell_\\infty$ 收斂到更穩定的值。為了避免與 Adam 混用，所以使用 $u_t$ \xa0 來表示無窮範數約束  $v_t$ \xa0 ： \n $$u_t = \\beta_2^\\infty v_{t-1} + (1 - \\beta_2^\\infty) |g_t|^\\infty$$ \n $$ = \\max(\\beta_2 \\cdot v_{t-1}, |g_t|) $$ \n 替換為 Adam 更新公式$\\sqrt{\\hat{v}_t} + \\epsilon$    和 $u_t$\xa0得出 AdaMax 更新規則： \n $$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{u_t} \\hat{m}_t$$ \n 注意 $u_t$ 依靠最大運算，不建議 Adam 中的  $m_t$ \xa0 和  $v_t$ \xa0 偏向零，這就是為什麼不需要針對  $u_t$  計算偏差。好的預設值 $\\eta = 0.002$， $\\beta_1 \xa0 = 0.9$ 和  $\\beta_2 \xa0= 0.999$。 \n Nadam \n Nadam (Nesterov-accelerated Adaptive Moment Estimation，Nesterov 加速的自適應矩估計)，結合 Adam 和 NAG。為了將 NAG 納入 Adam，需要修改動量項  $m_t$ 。使用先前符號回顧動量更新規則： \n $$g_t = \\nabla_{\\theta_t}J(\\theta_t)$$ \n $$m_t = \\gamma m_{t-1} + \\eta g_t$$ \n $$\\theta_{t+1} = \\theta_t - m_t $$ \n J是目標函數，$\\gamma$ 是動量衰減項，$\\eta$ 是 step size(學習率)，上面的第三個方程式擴展為： \n $$\\theta_{t+1} = \\theta_t - ( \\gamma m_{t-1} + \\eta g_t)$$ \n 再次證明了動量涉及在前一個動量向量的方向上往前一步和在當前梯度的方向上邁出一步。NAG 然後允許計算梯度之前透過更新動量步長參數使梯度方向上執行更精確的步長。因此，我們只需要修改梯度 $g_t$\xa0到達 NAG： \n $$g_t = \\nabla_{\\theta_t}J(\\theta_t - \\gamma m_{t-1})$$ \n $$m_t = \\gamma m_{t-1} + \\eta g_t$$ \n $$\\theta_{t+1} = \\theta_t - m_t$$ \n Dozat 建議修改 NAG：一次用於更新梯度  $g_t$ \xa0第二次更新參數 $\\theta_{t+1}$，直接應用先前的動量向量來更新當前參數： \n $$g_t = \\nabla_{\\theta_t}J(\\theta_t)$$ \n $$m_t = \\gamma m_{t-1} + \\eta g_t$$ \n $$\\theta_{t+1} = \\theta_t - (\\gamma m_t + \\eta g_t)$$ \n 為了將 Nesterov 動量添加到 Adam，可以類似地用當前動量向量替換以前的動量向量。回想一下 Adam 更新規則 如下： \n $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$ \n $$\\hat{m}_t = \\frac{m_t}{1 - \\beta^t_1}$$ \n $$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$ \n 用定義拓展第二個方程式： \n $$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\frac{\\beta_1 m_{t-1}}{1 - \\beta^t_1} + \\frac{(1 - \\beta_1) g_t}{1 - \\beta^t_1})$$ \n 注意 $\\frac{\\beta_1 m_{t-1}}{1 - \\beta^t_1}$ 只是前一個的 time step 的動量向量的偏差來校正評估。因此，可以將其替換為 $\\hat{m}_{t-1}$： \n $$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\beta_1 \\hat{m}_{t-1} + \\frac{(1 - \\beta_1) g_t}{1 - \\beta^t_1})$$ \n 為簡化，因為無論如何將在下一步中替換分母，所以忽略了分母 $1 - \\beta^t_1$。該方程式再次看起來和上面擴展的動量更 新規則非常相似。可以像以前一樣添加 Nesterov 動量，方法是用當前動量向量偏差校正後的評估值替換前一時間步 長的動量向量偏差校正後的評估值，這為我們提供了 Nadam 更新規則： \n $$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\beta_1 \\hat{m}_t + \\frac{(1 - \\beta_1) g_t}{1 - \\beta^t_1})$$ \n AMSGrad \n Reddi 等（2018）。正式化了這個問題，並指出了泛化行為不佳的原因：將過去梯度平方的指數移動平均值作為自適應學習率方法。雖然引入指數平均值的動機很充分：應防止學習率隨著訓練的進行而變得無限小；但這也是 Adagrad 算法的關鍵缺陷。在其他情況下，短期記憶的梯度成為障礙。 \n 在 Adam 收斂到次優解的環境中，已經觀察到一些小型批次提供了較大且信息豐富的梯度，但是這些小型批次很少出現，因此指數平均會減小其影響，從而導致收斂性較差。作者 (資料來源的作者) 提供了一個簡單的凸型優化問題的例子，其中 Adam 可以觀察到相同的行為。AMSGrad算法是為了解決此問題，這算法使用了過去梯度平方的最大值  $v_t$ \xa0 而不是指數平均值來更新參數。 $v_t$ \xa0 的定義與先前的 Adam 相同： \n $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$ \n 而不是直接使用 $v_t$(或其偏差更正的版本 vˆt），如果現在使用以前值的大於現在的值： \n $$\\hat{v}_t = \\text{max}(\\hat{v}_{t-1}, v_t)$$ \n 這方式 AMSGrad 不會增加步長 (step size)，從而避免了 Adam 遇到的問題。為了簡化，AMSGrad 去除了 Adam 的去偏差 (debias) 步驟。可以看到完整的 AMSGrad 更新，沒有經過偏差校正的估計： \n $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ \n $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$ \n $$\\hat{v}_t = \\text{max}(\\hat{v}_{t-1}, v_t)$$ \n $$\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} m_t$$ \n 在小型數據集和 CIFAR-10 上，與 Adam 相比，性能有所提高。但是，其他實驗顯示其性能與 Adam 相近或更 差。在實際運用，AMSGrad 是否能勝過 Adam，還有待觀察。 \n Gradient noise \n 增加 noise 跟隨高斯分布 $N(0, \\sigma^2_t)$\xa0對每個梯度更新： \n $$g_{t, i} = g_{t, i} + N(0, \\sigma^2_t)$$ \n 根據排定時間對差異計算： \n $$\\sigma^2_t = \\frac{\\eta}{(1 + t)^\\gamma}$$ \n 添加這種 noise 對不良初始化的網絡可使其強化，並有助於訓練特別深且復雜的網絡。他們懷疑增加的噪聲使模型 有更多的機會逃脫並找到新的局部極小值，這對於更深的模型而言更常見。 \n', 'tags': '', 'url': 'Gradient descent optimization algorithms.html'}, {'title': '程式', 'text': 'pygame \n 訓練用的Pygame程式： \n https://github.com/s40723150/pygame_airhockey \n https://github.com/mdecourse/4072pj1/tree/master/40723150/pygame \n Pong control \n 將pong1.2訓練好的模型丟回到gym，讓訓練好的模型和gmy內建AI對打。 \n 目前訓練結果平均值不會超過15分。 \n \n', 'tags': '', 'url': '程式.html'}, {'title': 'RL-Pong Game', 'text': 'import pygame\nimport random , sys\nfrom pygame.locals import *\nimport numpy as np\nfrom collections import deque\nimport tensorflow as tf  # http://blog.topspeedsnail.com/archives/10116\nimport cv2  # http://blog.topspeedsnail.com/archives/4755\n\ntf.compat.v1.disable_eager_execution()\n\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\n\nSCREEN_SIZE = [320, 400]\nBAR_SIZE = [50, 5]\nBAR2_SIZE = [50,5]\nBALL_SIZE = [15, 15]\n\n# 神經網絡的輸出\nMOVE_STAY = [1, 0, 0]\nMOVE_LEFT = [0, 1, 0]\nMOVE_RIGHT = [0, 0, 1]\n\n\nclass Game(object):\n    def __init__(self):\n        pygame.init()\n        self.clock = pygame.time.Clock()\n        self.screen = pygame.display.set_mode(SCREEN_SIZE)\n        pygame.display.set_caption(\'Simple Game\')\n\n        self.ball_pos_x = SCREEN_SIZE[0] // 2 - BALL_SIZE[0] / 2\n        self.ball_pos_y = SCREEN_SIZE[1] // 2 - BALL_SIZE[1] / 2\n\n        self.ball_dir_x = -1  # -1 = left 1 = right\n        self.ball_dir_y = -1  # -1 = up   1 = down\n        self.ball_pos = pygame.Rect(self.ball_pos_x, self.ball_pos_y, BALL_SIZE[0], BALL_SIZE[1])\n\n        self.bar_pos_x = SCREEN_SIZE[0] // 2 - BAR_SIZE[0] // 2\n        self.bar_pos = pygame.Rect(self.bar_pos_x, SCREEN_SIZE[1] - BAR_SIZE[1]-5, BAR_SIZE[0], BAR_SIZE[1])\n\n        self.bar2_pos_x = SCREEN_SIZE[0] // 2 - BAR_SIZE[0] // 2\n        self.bar2_pos = pygame.Rect(self.bar_pos_x, 5, BAR_SIZE[0], BAR_SIZE[1])\n        self.bar2_speed = 7\n\n\n    # action是MOVE_STAY、MOVE_LEFT、MOVE_RIGHT\n    # ai控制棒子左右移動；返回遊戲界面像素數和對應的獎勵。(像素->獎勵->強化棒子往獎勵高的方向移動)\n\n\n    def step(self, action):\n\n        if action == MOVE_LEFT:\n            self.bar_pos_x = self.bar_pos_x - 2\n        elif action == MOVE_RIGHT:\n            self.bar_pos_x = self.bar_pos_x + 2\n        else:\n            pass\n        if self.bar_pos_x < 0:\n            self.bar_pos_x = 0\n        if self.bar_pos_x > SCREEN_SIZE[0] - BAR_SIZE[0]:\n            self.bar_pos_x = SCREEN_SIZE[0] - BAR_SIZE[0]\n\n        if  self.bar2_pos.left < self.ball_pos.x:\n            self.bar2_pos.x += self.bar2_speed\n        if  self.bar2_pos.right > self.ball_pos.x:\n            self.bar2_pos.x -= self.bar2_speed\n\n        if  self.bar2_pos.left <= 0:\n            self.bar2_pos.left = 0\n        if  self.bar2_pos.right >= SCREEN_SIZE[0]:\n            self.bar2_pos.right = SCREEN_SIZE[0]\n\n        self.screen.fill(BLACK)\n        self.bar_pos.left = self.bar_pos_x\n        pygame.draw.rect(self.screen, WHITE, self.bar_pos)\n        pygame.draw.rect(self.screen, WHITE, self.bar2_pos)\n        pygame.draw.rect(self.screen, (255, 0, 0), Rect((5, 5), (310, 390)), 2)\n        pygame.draw.line(self.screen, (255, 0, 0), (7, 195), (313, 195), 2)\n        pygame.draw.circle(self.screen, (255, 0, 0), (160, 195), 50, 2)\n\n        self.ball_pos.left += self.ball_dir_x * 2\n        self.ball_pos.bottom += self.ball_dir_y * 3\n        pygame.draw.rect(self.screen, WHITE, self.ball_pos)\n\n        if self.ball_pos.top <= 0 or self.ball_pos.bottom >= (SCREEN_SIZE[1] - BAR_SIZE[1] + 1):\n            self.ball_dir_y = self.ball_dir_y * -1\n        if self.ball_pos.left <= 0 or self.ball_pos.right >= (SCREEN_SIZE[0]):\n            self.ball_dir_x = self.ball_dir_x * -1\n\n        reward = 0\n        if self.bar_pos.top <= self.ball_pos.bottom and (\n                self.bar_pos.left < self.ball_pos.right and self.bar_pos.right > self.ball_pos.left):\n            reward = 1  # 擊中獎勵\n        elif self.bar_pos.top <= self.ball_pos.bottom and (\n                self.bar_pos.left > self.ball_pos.right or self.bar_pos.right < self.ball_pos.left):\n            reward = -1  # 沒擊中懲罰\n\n        # 獲得遊戲界面像素\n        screen_image = pygame.surfarray.array3d(pygame.display.get_surface())\n        pygame.display.update()\n        # 返回遊戲界面像素和對應的獎勵\n        return reward, screen_image\n\n\n# learning_rate\nLEARNING_RATE = 0.99\n# 更新梯度\nINITIAL_EPSILON = 1.0\nFINAL_EPSILON = 0.05\n# 測試觀測次數\nEXPLORE = 500000\nOBSERVE = 50000\n# 存儲過往經驗大小\nREPLAY_MEMORY = 500000\n\nBATCH = 100\n\noutput = 3  # 輸出層神經元數。代表3種操作-MOVE_STAY:[1, 0, 0]  MOVE_LEFT:[0, 1, 0]  MOVE_RIGHT:[0, 0, 1]\ninput_image = tf.compat.v1.placeholder("float", [None, 80, 100, 4])  # 遊戲像素\naction = tf.compat.v1.placeholder("float", [None, output])  # 操作\n\n\n# 定義CNN-卷積神經網絡 參考:http://blog.topspeedsnail.com/archives/10451\ndef convolutional_neural_network(input_image):\n    weights = {\'w_conv1\': tf.Variable(tf.zeros([8, 8, 4, 32])),\n               \'w_conv2\': tf.Variable(tf.zeros([4, 4, 32, 64])),\n               \'w_conv3\': tf.Variable(tf.zeros([3, 3, 64, 64])),\n               \'w_fc4\': tf.Variable(tf.zeros([3456, 784])),\n               \'w_out\': tf.Variable(tf.zeros([784, output]))}\n\n    biases = {\'b_conv1\': tf.Variable(tf.zeros([32])),\n              \'b_conv2\': tf.Variable(tf.zeros([64])),\n              \'b_conv3\': tf.Variable(tf.zeros([64])),\n              \'b_fc4\': tf.Variable(tf.zeros([784])),\n              \'b_out\': tf.Variable(tf.zeros([output]))}\n\n    conv1 = tf.nn.relu(\n        tf.nn.conv2d(input_image, weights[\'w_conv1\'], strides=[1, 4, 4, 1], padding="VALID") + biases[\'b_conv1\'])\n    conv2 = tf.nn.relu(\n        tf.nn.conv2d(conv1, weights[\'w_conv2\'], strides=[1, 2, 2, 1], padding="VALID") + biases[\'b_conv2\'])\n    conv3 = tf.nn.relu(\n        tf.nn.conv2d(conv2, weights[\'w_conv3\'], strides=[1, 1, 1, 1], padding="VALID") + biases[\'b_conv3\'])\n    conv3_flat = tf.reshape(conv3, [-1, 3456])\n    fc4 = tf.nn.relu(tf.matmul(conv3_flat, weights[\'w_fc4\']) + biases[\'b_fc4\'])\n\n    output_layer = tf.matmul(fc4, weights[\'w_out\']) + biases[\'b_out\']\n    return output_layer\n\n\n# 深度強化學習入門: https://www.nervanasys.com/demystifying-deep-reinforcement-learning/\n# 訓練神經網絡\ndef train_neural_network(input_image):\n    predict_action = convolutional_neural_network(input_image)\n\n    argmax = tf.compat.v1.placeholder("float", [None, output])\n    gt = tf.compat.v1.placeholder("float", [None])\n\n    action = tf.reduce_sum(tf.multiply(predict_action, argmax), axis=1)\n    cost = tf.reduce_mean(tf.square(action - gt))\n    optimizer = tf.compat.v1.train.AdamOptimizer(1e-6).minimize(cost)\n\n    game = Game()\n    D = deque()\n\n    _, image = game.step(MOVE_STAY)\n    # 轉換爲灰度值\n    image = cv2.cvtColor(cv2.resize(image, (100, 80)), cv2.COLOR_BGR2GRAY)\n    # 轉換爲二值\n    ret, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\n    input_image_data = np.stack((image, image, image, image), axis=2)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.initialize_all_variables())\n\n        saver = tf.compat.v1.train.Saver()\n\n        n = 0\n        epsilon = INITIAL_EPSILON\n        while True:\n            for event in pygame.event.get():\n             if event.type == pygame.QUIT:\n                 pygame.quit()\n                 sys.exit()\n\n\n            action_t = predict_action.eval(feed_dict={input_image: [input_image_data]})[0]\n\n            argmax_t = np.zeros([output], dtype=np.int)\n            if (random.random() <= INITIAL_EPSILON):\n                maxIndex = random.randrange(output)\n            else:\n                maxIndex = np.argmax(action_t)\n            argmax_t[maxIndex] = 1\n            if epsilon > FINAL_EPSILON:\n                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n\n            # for event in pygame.event.get():  macOS需要事件循環，否則白屏\n            #\tif event.type == QUIT:\n            #\t\tpygame.quit()\n            #\t\tsys.exit()\n            reward, image = game.step(list(argmax_t))\n\n            image = cv2.cvtColor(cv2.resize(image, (100, 80)), cv2.COLOR_BGR2GRAY)\n            ret, image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)\n            image = np.reshape(image, (80, 100, 1))\n            input_image_data1 = np.append(image, input_image_data[:, :, 0:3], axis=2)\n\n            D.append((input_image_data, argmax_t, reward, input_image_data1))\n\n            if len(D) > REPLAY_MEMORY:\n                D.popleft()\n\n            if n > OBSERVE:\n                minibatch = random.sample(D, BATCH)\n                input_image_data_batch = [d[0] for d in minibatch]\n                argmax_batch = [d[1] for d in minibatch]\n                reward_batch = [d[2] for d in minibatch]\n                input_image_data1_batch = [d[3] for d in minibatch]\n\n                gt_batch = []\n\n                out_batch = predict_action.eval(feed_dict={input_image: input_image_data1_batch})\n\n                for i in range(0, len(minibatch)):\n                    gt_batch.append(reward_batch[i] + LEARNING_RATE * np.max(out_batch[i]))\n\n                optimizer.run(feed_dict={gt: gt_batch, argmax: argmax_batch, input_image: input_image_data_batch})\n\n            input_image_data = input_image_data1\n            n = n + 1\n\n            if n % 10000 == 0:\n                saver.save(sess, \'game.cpk\', global_step=n)  # 保存模型\n\n            print(n, "epsilon:", epsilon, " ", "action:", maxIndex, " ", "reward:", reward)\n\n\ntrain_neural_network(input_image) \n', 'tags': '', 'url': 'RL-Pong Game.html'}, {'title': 'Pong control', 'text': '""" Majority of this code was copied directly from Andrej Karpathy\'s gist:\nhttps://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n# https://raw.githubusercontent.com/omkarv/pong-from-pixels/master/pong-from-pixels.py\n\n Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. """\nimport numpy as np\nimport pickle\nimport gym\nimport time as t\nfrom gym import wrappers\n\nH = 200 # number of hidden layer neurons\nbatch_size = 10 # used to perform a RMS prop param update every batch_size steps\nlearning_rate = 1e-3 # learning rate used in RMS prop\ngamma = 0.99 # discount factor for reward\ndecay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n\nresume = 1\nrender = 1 # render video output\n\n# model initialization\nD = 75 * 80 # input dimensionality: 75x80 grid\nif resume:\n  model = pickle.load(open(\'pong1.2_save.p\', \'rb\'))\nelse:\n  print("Resume Error!!")\n\ndef sigmoid(x):\n  return 1.0 / (1.0 + np.exp(-x)) # sigmoid "squashing" function to interval [0,1]\n\ndef prepro(I):\n  """ prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector """\n  I = I[35:185] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n  I = I[::2,::2,0] # downsample by factor of 2.\n  I[I == 144] = 0 # erase background (background type 1)\n  I[I == 109] = 0 # erase background (background type 2)\n  I[I != 0] = 1 # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n  return I.astype(np.float).ravel() # ravel flattens an array and collapses it into a column vector\n\ndef policy_forward(x):\n  """This is a manual implementation of a forward prop"""\n  h = np.dot(model[\'W1\'], x) # (H x D) . (D x 1) = (H x 1) (200 x 1)\n  h[h<0] = 0 # ReLU introduces non-linearity\n  logp = np.dot(model[\'W2\'], h) # This is a logits function and outputs a decimal.   (1 x H) . (H x 1) = 1 (scalar)\n  p = sigmoid(logp)  # squashes output to  between 0 & 1 range\n  return p, h # return probability of taking action 2 (UP), and hidden state\n\nenv = gym.make("Pong-v0")\n\nobservation = env.reset()\n\nprev_x = None # used in computing the difference frame\nxs,hs,dlogps,drs = [],[],[],[]\nrunning_reward = None\nreward_sum = 0\nepisode_number = 0\nwhile True:\n  if render: env.render()\n  cur_x = prepro(observation) #丟入畫面\n  x = cur_x - prev_x if prev_x is not None else np.zeros(D)#兩張畫面相減\n  prev_x = cur_x\n  aprob, h = policy_forward(x)\n  action = 2 if 0.5 < aprob else 3\n  xs.append(x) # observation\n  hs.append(h) # hidden state\n  y = 1 if action == 2 else 0\n  dlogps.append(y - aprob)\n  observation, reward, done, info = env.step(action)\n  reward_sum += reward\n  drs.append(reward)\n\n  if done:\n    episode_number += 1\n\n    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n    print (\'resetting env. episode reward total was \' + str(reward_sum) + \' running mean: \'+ str(running_reward))\n\n    reward_sum = 0\n    observation = env.reset() # reset env\n    prev_x = None\n\n  if reward != 0:\n    print ((\'round %d game finished, reward: %f\' % (episode_number, reward)) + (\'\' if reward == -1 else \' !!!!!!!!\')) \n \n \n', 'tags': '', 'url': 'Pong control.html'}, {'title': 'Ref', 'text': '', 'tags': '', 'url': 'Ref.html'}, {'title': 'Flutter', 'text': 'Flutter 與 Android sdk 版本差異問題解決:  https://stackoverflow.com/questions/60440509/android-command-line-tools-sdkmanager-always-shows-warning-could-not-create-se/61613986#61613986 \n Flask 與 ML 系統在後端, Flutter 作為前端 \n https://medium.com/analytics-vidhya/deploy-ml-models-using-flask-as-rest-api-and-access-via-flutter-app-7ce63d5c1f3b \n https://medium.com/@pyzimos/flutter-chatbot-with-python-flask-backend-heroku-deployment-706baafbb8f1 \n https://github.com/SHARONZACHARIA/Deploy-ML-model \n https://github.com/tonynguyen72/Flask_API_Flutter \n https://github.com/mohammedhashim44/Flutter-Flask-Login \n', 'tags': '', 'url': 'Flutter.html'}, {'title': 'CMSiMDE', 'text': 'https://websitesetup.org/bootstrap-tutorial-for-beginners/ \n https://colorlib.com/wp/themes/travelify/ \xa0 \n https://github.com/puikinsh/travelify \n \n', 'tags': '', 'url': 'CMSiMDE.html'}, {'title': 'Q&A', 'text': 'MathJax \n ffmpeg \n dll \n', 'tags': '', 'url': 'Q&A.html'}, {'title': 'MathJax', 'text': 'Q：啟用cmsimde的MathJax的功能遇到文章使用括號補充說明的內容被誤當成latex的語法轉換。 \n A：格式轉換原始定義成"("和")"，所以出現誤換的問題 \n 原程式： \n <script>\n  MathJax = {\n    tex: {inlineMath: [[\'$\', \'$\'], [\'\\\\(\', \'\\\\)\']]}\n  };\n  </script>\n  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script> \n 修正後將 "("和")"換成"$"，就解決誤換問題 \n 修正後： \n <script>\n  MathJax = {\n    tex: {inlineMath : [ [\'$\',\'$\'], ["\\\\$","\\\\$"] ]}\n  };\n  </script>\n  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>  \n', 'tags': '', 'url': 'MathJax.html'}, {'title': 'ffmpeg', 'text': 'Q：錄製訓練過程的程式讀不到ffmpeg。 \n \n 圖1.程式讀不到ffmpeg \n A：需要在作業系統中安裝ffmpeg \n \n 下載、解壓縮 先到官網\xa0 https://ffmpeg.org/download.html \xa0下載 " Windows builds from gyan.dev "，下載 https://www.gyan.dev/ffmpeg/builds/ffmpeg-git-full.7z ，解壓縮重新命名成"ffmpeg"並放到C槽目錄下(C:\\ffmpeg)。 \n 環境設定(windows10 20H2 及 2004版本) 開啟"設定"→"系統"→左方"關於"選項→右側"進階系統設定"→"環境變數"(圖2.)→選取"Path"，編輯(圖3.)→"新增"，增加一個環境變數，給定內容為:"C:\\ffmpeg\\bin"，"確定"(圖4.)→"確定"→"確定 \n \n 圖2.進階系統設定 圖3.環境變數 圖4.編輯環境變數 \n \n 測試 開啟命令字元(win+R，輸入"cmd")，執行"ffmpeg"(圖5.) \n \n \n 圖5.ffmpeg成功執行 \n', 'tags': '', 'url': 'ffmpeg.html'}, {'title': '動態連結庫', 'text': 'Q：gym 用到的 atari 動態連結庫在讀取目錄下但在執行的時候出現缺少"ale_c.cp38-win_amd64.dll" \n', 'tags': '', 'url': '動態連結庫.html'}]};